{
  "id": "robotic_grasp_detection_using_a_novel_two-stage_approach_",
  "title": "Robotic grasp detection using a novel two-stage approach ⋆",
  "authors": [
    "ZheChu",
    "MengkaiHu",
    "XiangyuChen"
  ],
  "content": "Robotic grasping is a core function for intelligent robot to perform a variety of autonomous manipulation tasks [6]. Humans can instinctively perceive the unstructured environment, find out the characteristics of the grasped object and grasp the object directly. But the robot can't do that and if we model the surrounding environment around the robot, it is not only time-consuming but also hard to model. Recently, with the development of deep learning,many end-to-end robotic grasp detection approaches based on CNN are developed. Most of these new approaches are one-stage approaches, which find a good grasp in one step using CNN and get a high accuracy on the datasets. But most of these end-to-end approaches [1][5] [10] get the results in one step using trained CNN models, so those approaches have very high request to accuracy of the model which relies heavily on the quality of datasets. But in practical use, it's very hard to make a high-quality dataset. Hence, in order to garner better results in practice, we need to find a robotic grasp detection approach which has a lower request to accuracy of the model. Sliding window detection is a robotic grasp detection which was often used in the past. The approach uses the classifier to test the selected parts of the image one by one. The part which has the highest score is considered to be the best grasp of the object [10]. Although this approach is a little time-consuming, it can take measures such as randomization, conditional constraints and so on to reduce the influence of model's inaccuracy on the final result. Based on sliding window detection and CNN, we propose a two-stage robotic grasp detection approach which can extract the best grasp from the object's RGB image using PSO candidate estimator and CNN. First, we design a robotic grasp identification model based deep CNN to determine ⋆ We gratefully acknowledge the assistance of Haobin Shi and Guosheng Huang giving us some guidance. * Corresponding author chxy95@mail.nwpu.edu.cn (X. Chen) whether the input is a good grasp. Then, we find good candidate grasps using candidate estimator. This candidate estimator finds proper candidate grasps using PSO algorithm. Using this approach, we lower the requirement of the model accuracy. We evaluate our approach on the Cornell Grasp Detection Dataset (see Figure 1). Our approach achieved an accuracy of 92.8%. This paper is organized as follows: in section 2, we discuss the related work. In section 3, we present our identification models. In section 4, we describe our candidate estimator. In section 5, we present our experiment and evaluation. In section 6, We present our results and then compare it with other approaches. In section 7, we present our experiment on real robots. Finally, we provide our conclusions. In the past few decades, with the development of robots and artificial intelligence, a number of robotic grasp detection approaches have been produced. Most of the early approaches depend on the accurate information about environment and object to find good grasps. Based on accurate information, some approaches [2][7] [8][9] are used to realize successful and stable grasps. But the disadvantage of these approaches is that many robots usually don't know beforehand the model of the grasp object, so it's hard to build the complex 3D models of the objects. Recent years, with the development of deep learning, many researchers propose to detect the grasp using deep learning. Here are the known approaches. Ian Lenz proposed a two-stage approach based on sliding window detection [6]. First, they find some candidate grasp rectangles. Then they test all these candidate grasp rectangles using a CNN with simple structure to get a set of grasps with high probability. Finally, they use a complex CNN to rank all the grasps which are screened out for the first stage and find the best grasp of the object. Because the sliding window detection is too time-consuming, Zhichao Wang proposed a novel robotic grasp detection system [11]. In this system, they first reuse the result of the former grasping detection for feedback to find candidate grasp with higher probability. Then they use a CNN to evaluate the candidate grasps and find the best grasp. Joseph Redmon proposed a new CNN model by simplifying AlexNet network [10]. Using this model, they can directly get a regression to the grasp and the classification of the grasp object. With the development of deep learning, Sulabh Kumra proposed a novel CNN model for robotic grasp detection [5]. This model consists of two 50-layer CNNs and a shallow CNN. They merge the outputs of these two 50layer CNNs and input them into the shallow CNN to predict the grasp configuration. Among these approaches, Ian Lenz and Zhichao Wang's approaches are two-stage, but the accuracy is not good (see Table 1). Joseph Redmon and Sulabh Kumra's approaches are end-to-end,which have high requirements for the accuracy of the dataset. It's hard to achieve in practical use. So we decide to attempt a better two-stage approach. We use CNN as the grasp identification model. In order to enable the identification model to learn how to distinguish between grasping and non-grasping features, we first train the network as a binary classifier. After the model training, the classification probability of the softmax layer is taken as the model output. In order to reduce the computing cost and increase the computing speed, we used smaller networks and tried the following models. Our first model is based on a simplified version of Alex-Net proposed by Krizhevsky. AlexNet achieved high accuracy on target recognition tasks. Our first model used the AlexNet architecture, but changed the configuration. The model (see Figure 2) includes 3 convolutional layers,3 maximum pooling layers, and 2 fully connected layers. Finally, softmax activation function is used to activate the output. We trained the model. And the highest training accuracy of the model is 83.39%, and the highest validation accuracy is 82.12%, which is not ideal. We use the method of feature map visualization to find the problems existing in the model. We output the feature map behind each layer (see Figure 3). From the input image on the far left of figure 3, we can see that the key features of the image should be located in the middle of the image. But in the end the critical feature our model learned (the yellow part of the image) was to the left of the image. This shows that our model only extracts part of the texture details, while the critical features of the image are not extracted. In addition, due to the existence of pooling layer, the further back, the less image information will be retained. Based on the problems with Simplified Alexnet model, we decided to try using a more complex model to extract deep, critical features. We increase the size and number of convolution kernels. In the new model (see Figure 4), the size of the convolution kernel of the three convolutional layers is 3, 5 and 7, and the number is 32,64,128, respectively. In addition, we remove the Maxpooling layer after each convolutional layer to retain more information. And we add the Batch Normalization operation between the convolution and activation functions of the convolution layer to speed up convergence. After the two fully connected layers, we use the softmax activation function to output. Finally, we select Relu as the activation function of the convolution layer. We trained the model and achieved 98.50% train accuracy and 85.50% validation accuracy. It is obvious that the model has the phenomenon of overfitting. And this modelis time-consuming, it takes four times as long as the first model. In addition, during the process of feature map visualization (see Figure 5), we found that Original GraspingNet model was still not so ideal in extracting critical features. As can be seen from Figure 5, the critical features extracted by the model are still to the left of the picture. We modified our identification model based on the former 2 models. A CNN usually consists of convolution layer, pooling layer and full connection layer. But in Final Grasp-ingNet model, we also remove the pooling layer and add the Batch Normalization operation between the convolution and activation functions of the convolution layer. Then, we remove the full connection layer with a large number of participants and use the GAP layer instead. By doing so, on the one hand, it can prevent overfitting and make the classification more natural; on the other hand, it can also increase the speed of the model. So our final identification model is an 8-layer CNN, which is mainly composed of Convolution layer, Global Average Pooling (GAP) layer and the output layer (see Figure 6). In this model, we decide to replace the large convolution kernels of Original GraspingNet with 3 by 3 convolution kernels so that it can improve the ability of the model to extract critical features while increasing the speed of the model. In each convolution layer, the model needs to perform convolution, Batch Normalization, and Relu activation function operations in turn, before being input to the next convolution layer. In the first convolution layer, we use 32 3*3 convolutional kernels to calculate with stride 1, and ignore the pixels of the image edge by setting padding=valid. After the convolution operation, we'll do the Batch Normalization and Relu activation function operations. The operation of the first convolution layer is defined as 1 (.), the input is defined as 1 , and the output is defined as 1 . Then the operation of the first convolution layer is as follows: In the second, third and fourth convolution layers, we use 64 3*3 convolution kernels with stride 2 for operation. After the convolution operation, we'll do the Batch Normalization and Relu activation function operations. This operation is defined as 2 (.), and the output of the second, third and fourth layer is defined as 2 , 3 , 4 , then the operation of the second, third and fourth convolution layer is as follows: (2) In the last four convolution layers, we use 128 3*3 convolution kernels to operate with stride 2. This operation is defined as 3 (.), and the output of the last four convolution layers is defined as 5 , 6 , 7 , 8 , then the operation of the last four convolution layers is as follows: (5) The Global Average Pooling operation conducted by GAP layer is defined as (.), and the output is defined as , then, Before the final output of the model, we first use the convolution of 1*1 to perform ascending and reducing operations on the output of the GAP layer to increase the information combination across channels. In addition, we will use dropout to further prevent overfitting of the model before ascending and reducing operations. We define the dropout operation as (.), the 1*1 convolution ascending dimension operation as (.) and the output of the dropout operation and ascending dimension operation as , then, We define 1*1 convolution reducing operation as (.) and the output of the dropout operation and ascending dimension operation as , then, Finally, we activate it using the softmax function, which outputs the results of the model identification in probabilistic form. We define the operation of the softmax function as (.) and the output of the output layer as , then, = ( ) (12) The training process of the convolutional neural network model is to find the global optimal solution in the parameter space. In this process, we may encounter many local optima, but we have to skip the local optima to find the global optima in the parameter space to train the best model. Therefore, we used the stochastic gradient descent method to train the CNN model and set the learning rate in sections. During training (see Figure 7), the learning rate decayed every 60 epochs. In this way, when the model starts training, because the training speed is faster, some local optima are skipped. Finally, when the model is trained near the optimal solution, the learning rate converges to the optimal solution of the model at a very low learning rate. In addition, during the training, we also increased the value of momentum. On the one hand, it is more helpful for the model to get rid of the local optimal solution, on the other hand, it can also increase the stability of model training to some extent. There are mainly two kinds of grasp representation. One is a seven-dimensional representation which is proposed by Jiang Yun [12]. The other is a five-dimensional representation which is proposed by Ian Lenz [6]. The five-dimensional representation is a simplification of the seven-dimensional representation. In order for computational convenience, we use the five-dimensional representation. In the five-dimensional representation, the grasp is a rectangle which is determined by location, size and direction as: = { , , , ℎ, } ( , ) is the location of a rectangle, is the direction of the rectangle relative to the horizontal axis,h is the length of the gripper,w is the width of the gripper opening(see Figure 8). In the sliding window detection, every image to be detected is a large and continuous state space. If we use enumeration approach to search the grasp, we need to detect a great number of candidate grasps. For example, if we detect an image whose size is 224 by 224, we may need to consider 50176(224*224=50176) locations. Suppose that there are 70 different lengths of grippers, the width of the gripper opening ranges from 30 to 100 and that the angle of the gripper ranges from 0°degree to 180°and the angle changes in one degree, we should consider 632,217,600(50176*70*180 = 632,217,600) candidates. If we do that, the cost of computing is very large. In order to solve this problem, we design a candidate estimator based on PSO. This estimator transfers the search of the grasp to an optimization problem. The parameters of the optimization problem are the five-dimensional representation of every candidate. The processed image is the state space of the optimization problem and the constraint of the problem is the edges of the image. Our optimization goal is to find a candidate to optimize the score function of the grasp. As for the score function, we use the grasp identification model in section 3. As can be seen above, the grasp identification model outputs the probability of grasping. Then, we use PSO to solve the problem. PSO is a kind of stochastic optimization technique based on population, which was proposed by Eberhart and Kennedy in 1995. PSO algorithm mimics the clustering behavior of animals such as insects, herds, birds, and so on. These populations search for food in a cooperative way. Every particle of the population constantly changes the way it searches by learning from its own experience and that of other members. During the search of the algorithm, each particle searches for the optimal solution separately in the search space, and marks it as the current individual extremum. Then, the particle shares its individual extremum with other particles in the particle swarm and the particle swarm finds the optimal individual extremum as the current global optimal solution. All particles of the particle swarm adjust their speed and position according to their current individual extremums and the current global optimal solution shared by the particle swarm. Therefore, the algorithm can be used to deal with the optimization problem of multivariable functions with multiple local optima. In this article, we use a particle to represent a candidate. Every particle has a speed at iteration . The movement of the particle is determined by its current individual extremum and the current global optimal solution . The updated formula of particle velocity and position is as follows: is the inertia factor. 1 and 2 are accelerating factors. 1 and 2 are 2 random numbers, which are used for increasing the randomness of the search. The is shown in Algortithm 1. Randomly initialize particle swarm , ; 4: Use identification model to caculate score for , ; is the minimum threshold that _ should meet during searching.Through this candidate estimator, we can find a good grasp fast. The iterative convergence process of particles is as follows (see Figure 9): the particles are randomly distributed inside the image during initialization. And as the algorithm iterates, the particles gradually gather near the optimal location. If the center of a particle is near the edges of the image during initialization or iteration so that some components of the particle exceed the boundaries of the image when it is segmented, the particle will be eliminated and we will add a new particle randomly. The convergence condition is that the number of iterations exceeds the maximum number of iterations or the global optimal particle adaptation value reaches the threshold. And if we make a small change to candidate estimator, the estimator can output the particles whose scores are above a certain threshold and rank high among all the particles at the same time so that we can multiple grasps. In order to make the PSO algorithm converge faster, we add some constraints to the particles during initialization or iteration. First, we hope that the particles are distributed as close to the object as possible. So when initializing particles, we distribute the particles within the center of the image and there has to be a particle with a score greater than 0.7. Second, we get the histogram of gray image corresponding to the RGB image before initializing the particles. According to the histogram which we get, we can roughly estimate the size of the target object. Then, we will initialize the particles differently according to the size of the target object. We can make sure the particle size is proper and reduce the effect of unproper particle size on particle score so that we can accelerate the convergence of the algorithm. Finally, we limit the size, aspect ratio and area of particles to a certain extent. If a particle is out of the limit during initialization or iteration, we will get the particle size back to a reasonable range by multiplying it by a corresponding correction factor. The dataset contains 885 images, 400 objects, 240 different categories and corresponding grasp labels. This data set is specially designed for parallel gripper. Each image contains multiple grasp labels. The labels are comprehensive and varied in orientation, location and scale, but it can't contain all possible grasps. And there are also errors in the labeling of partial positive samples and negative samples. Nonetheless, they are excellent examples of grasps. So we choose to analyze and evaluate based on the dataset. Before inputting the image into the identification model, we preprocess the image. We clipped the original image to the center of the object, keeping 300 by 300 pixels. Then in order to compare with the previous work, we scaled the image to 224 by 224 pixels. As for the grasp identification model, we set the rectangular image horizontally along the long axis, fill the top and bottom ends with 0 pixels scale the image to 24 by 24 pixels before inputting the feature image to the model. Before training the model, we made necessary augmentation of the training set, including some operations such as translation, scaling, and rotation. Before the input of the feature image into the model, we will preprocess the image, scale the image to 24*24 and fill it. So we can input the preprocessed feature image directly. And our identification model outputs in the form of probability. And the output is probability of the network softmax layer classification of the graspable features (see Figure 10). If we use direct classification, take the third example in Figure 10. The model will classify it as graspable because the output is 0.617 which is greater than 0.5. If the model only outputs in the form of probability, it will not impact on subsequent iterations. So we don't directly evaluate models by how accurate they are, and it can be used as a reference for selecting identification model. We have made a small modification to the original method. Instead of only printing the highest-scoring grasps, we output several of the highest-scoring grasps at the same time. These grasps converge to different positions of the target object, so we can have multiple ways to grab the same object. If we evaluate on the Cornell Grasp Detection Dataset, there have been two different evaluation criteria. The first approach is point evaluation, which evaluates whether the predicted grasp is a successful grasp through judging whether the distance between the center of the predicted fetch point and the center of the tag is below a certain threshold. There has been a lot of discussion about this evaluation criteria. The biggest problem with this evaluation criteria is that it doesn't take into account the size and angle of the grasp. And under this evaluation criteria, previous work has rarely revealed thresholds for evaluating comparisons. So it is difficult to compare results. Therefore, we do not use such evaluation criteria. The second approach is rectangle evaluation. This approach considers the whole grasp rectangle. In this evaluation criteria, satisfying the following two points is considered to be a correct grab point: (1) The angle between the grasp and the graspable label should not exceed 30 • . (2) The intersection ratio of grasp and the graspable label should be no less than 20%. In our work, we use the second approach to evaluate the model. Although we use a more proper approach to evaluate, there are still some problems. Although some predicted grasps are not evaluated to meet the above two points, they are actually graspable(see Figure 11). The first line is the grasps labeled by the dataset, and the second line is the result of our detection. Take the results of column 1 and column 4 from the left in the picture as an example. Although they do not meet the requirements of rectangle evaluation, it can still be considered to be graspable. We tested on a computer configured with a single CPU (i9-9900k 3.6G), a single GPU (NVDIA 1080Ti), and 16 gigabytes of memory. According to the evaluation criteria in part iii, we achieved good results in the Cornell dataset. We compared the accuracy of our method with the results of previous experiments on the Cornell dataset (as shown in Table 1). Our approach achieved a success rate of 92.8%. Among all known approaches, our success rate is only slightly lower than Guo et al. Compared to the existing two-stage method, our method improved the success rate by 11 percentage points. Table 1 We compare our approach with the previous work. Accuracy(%) Jiang et al. [12] 60.5% Lenz et al. [6] 73.9% Wang et al. [11] 81.8% Redmon et al. [10] 88.0% Asif et al. [1] 88.2% Kumra et al. [5] 89.2% Guo et al. [3] 93.2% Our 92.8% In terms of speed, our algorithm needs 378ms to process an image. Although our method is still much slower than the one-stage method of Kumra et al. [5] and Redmon et al. [10], it should be able to run in real time. In the front, we mentioned that with a small change in our method, we can get multiple grab points at once, which can provide multiple solutions for grabbing an object. In this regard, also carried out relevant experiments. The results are seen in Figure 12. From the figure, we can see that the multigrasp method provides multiple correct grasps for each target object, and we also make statistics on the success rate and time consumption (see Table 2). In the evaluation of multigrasp detection, as long as one predicted grasp candidate of an image meets the requirements, we consider that the detection of capture points of this image is successful. It can be seen from the results that compared with the original method, the method of mutligrasp has a higher success rate. The increase in average time was only 5ms. In the multigrasp approach, we output multiple grasps at different locations. The fault tolerance of the identification model is further improved, because even if the model erroneously judges an ungraspable candidate to be the highest score of a target object, it is possible to find a correct grasp candidate from other grasps in the output. We show that deep learning model can better learn grasping features, and PSO optimization algorithm can well solve the problem of multi-local optimal value. We used an uncomplicated network, greatly reducing computing costs and making the model easy to train and deploy. We turn the detection problem into an optimization problem for processing, so that the detection result is not too dependent on label data, but has multiple possibilities in the whole state space. We use PSO algorithm to deal with this kind of multi-variable complex function optimization problem with multi-local optimal advantages, which has low computational complexity, fast convergence speed and can find a better solution in considerable time. In the problem of grasp detection, we combined deep network learning characteristics and PSO optimization algorithm to solve the problem quickly, and obtained very advanced results. After our approach worked well on the lab computer, we decided to transplant it to a real robot for testing. The process is as follows (see Figure 13): first, we selected six objects that did not appear in the dataset. Then, we use the watershed algorithm to process the original image taken in the robot camera to extract the target object, and place the extracted object in the same position on the light white background image. Finally, we use our method to detect the capture point of the image obtained in the previous step, and draw the grasp on the original image. We used our method to detect the grasping points on the robot and obtained the following results (see Figure 14): While there are no major differences in kind between the objects we select and those in the Cornell dataset, there are significant differences in detail. However, this did not have a great impact on our detection results. Among the six selected objects, except the pink plastic blocks in the second row and the first column, the other predicted grasps were correct. This shows that our detection approach can not only be used on the robot, but also achieve good results in the case that the unfamiliar object is not marked, as long as the object in the data set belongs to the same category as the unfamiliar object. In practical use, we can select a few typical objects in the same kind of objects to mark accurately, so as to achieve a good effect in the detection of grasping points of most objects in this class, thus reducing the number of objects required by the data set, conducive to the application of our method in the actual detection. In this article, we present a robotic grasp detection approach based on deep learning and PSO algorithm. We transfer the detection problem to an optimization problem. We first use the CNN to learn graspable feature. Then we take the identification model as the objective function of optimization, take the whole image as the state space, take the parameters of the rectangle identification points as variables and use the PSO algorithm to solve the optimization problem. In terms of accuracy, our algorithm is at the top of the Cornell Grasp Detection Dataset. Moreover, our approach is two-stage. Compared to end-to-end approach, it has a lower request to accuracy of the model and the dataset, which contributes to our practical use. In the future work, we will improve the speed and robustness of the algorithm so that it can achieve better results in real industrial deployment. All the codes used for experiment can be downloaded at https://github.com/KathylinLawes/Robotic-grasp-detection. And the color should be used for all figures in print.",
  "filePath": "C:\\Users\\Alex Isaev\\Documents\\truth source\\citation-verifier\\src\\document-database\\documents\\robotic_grasp_detection_using_a_novel_two-stage_approach_.json",
  "sourcePdf": "C:\\Users\\Alex Isaev\\Documents\\truth source\\citation-verifier\\src\\document-database\\pdf-documents\\35.pdf",
  "doi": "7965DB07E42E7852B077669D1E52D39D",
  "year": "2020",
  "journal": ""
}