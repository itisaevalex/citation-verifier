{
  "id": "prediction_of_creaky_speech_by_recurrent_neural_networks_using_psychoacoustic_ro",
  "title": "Prediction of Creaky Speech by Recurrent Neural Networks Using Psychoacoustic Roughness",
  "authors": [
    "Senior Memb, IEEEJuliánVillegas",
    "Memb, IEEEKonstantinMarkov",
    "Member, IEEEJeremyPerkins",
    "SeunghunJLee",
    "JulianVillegas",
    "JVillegas"
  ],
  "content": "Moreover, in other languages creakiness has been associated with marking parenthetical information [8], conforming with a specific demographic group's way of speaking [9], etc. The focus of this research is primarily on the phonemic role of creakiness, i.e., on how it is used for distinguishing words. From a physiological perspective, there seems to be a variety of ways to produce this phonation, and some authors have suggested that rather than a single phenomenon, creakiness is a set of differently produced phenomena [10]. Speakers who routinely use creaky speech may differ in the way they produce it, but ultimately, they are capable of successfully modifying their production in such a way that their interlocutors are able to distinguish creaky from non-creaky speech. Blomgren et al. [11] reported that listeners were capable of classifying modal and creaky (fry) utterances with accuracy ≥ 95.5% (1100 responses for each kind of phonation). Additional evidence supporting the idea that listeners can distinguish between modal and creaky phonation regardless of how it is produced is provided by Gerratt and Kreiman [12]. Thus, perceptual attributes of speech, as opposed to unprocessed acoustic attributes or physiological correlates, could be good predictors of creakiness. Among perceptual attributes of sound in general, psychoacoustic roughness seems to be related to the perception of creakiness. In the literature, both terms have been used to describe the other. For example, Titze posed that \"creaky voice seems to be perceived as some combination of low pitch and roughness\" [13]. Conversely, when describing the relationship between roughness and the number of audible beats, Helmholtz [14, p. 171] mentioned that \"slow beats give a coarse kind of roughness which can be described as rattling or jarring.\" In alternative translations of the German word 'knarrend,' the term 'creaking' has been used instead of 'jarring' [15]. Additionally, De Bruijn and Whiteside found that roughness and creakiness are strongly correlated when used for voice quality evaluation. This correlation was observed in ratings of language therapists and seems unaffected by differences in their years of experience [16]. Despite these commonalities, note that psychoacoustic roughness is a prothetic sensation (i.e., a percept on a continuum) [17], whereas creakiness is a metathetic sensation (i.e., a categorical percept) [12], so the two perceptual attributes are different, and in this study the former was used to predict the latter. Also note that in the framework of the Consensus Auditory-Perceptual Evaluation of Voice (CAPE-V) [18], the term 'roughness' was used as one of six voice quality features, and it was defined as \"perceived irregularity in the voicing source.\" In this article, however, we use 'roughness' to refer to the psychoacoustic feature exclusively. Recently, roughness has been linked to the perception of dysphonic voice [19]. The authors of that study found that psychoacoustic roughness models used to predict roughness of sinusoids, narrowband noise, etc., could also model reported roughness of dysphonic voice. In this study we investigated the possible role of psychoacoustic roughness in the perception of creakiness. To the extent of the authors' knowledge, this is the first study relating these phenomena. Concretely, the purpose of this research is two-fold: 1) to investigate the feasibility of psychoacoustic roughness models to predict creaky intervals in speech, and 2) to compare roughnessbased predictions with those made by state-of-the-art predictors. An automatic predictor of creakiness based on psychoacoustic roughness could provide a better understanding of the use of phonation as a means of contrast in some languages since it focuses on the reception/feedback end of the speech chain [20], rather than on the production side. Furthermore, it could also help to improve current detection methods of non-modal phonation, for the same reasons. The rest of the article is organized as follows: Section II discusses the current understanding of creaky phonation and psychoacoustic roughness, as well as some of their prediction models. Section III describes the corpus used in our research and discusses the feasibility and accuracy of roughness as a creaky predictor. In Section IV, the results of our experiments are discussed along with future work. Section V concludes the manuscript with a summary of our main findings. Creaky phonation has been considered in some accounts as a point along a continuum between a fully open glottis (breathiness/voicelessness) and a fully closed glottis (glottal stop/closure) [21]- [24]. In this school of thought, such phonation is produced when the vocal folds have weak longitudinal tension while greatly adducted (i.e., arytenoid cartilages pressed together), thereby reducing the overall glottal opening and contributing to a slow and sometimes irregular vibration of the vocal folds [25]. Compared to modal voice, this glottal constriction is also reflected in a lower rate of airflow and Open Quotient (OQ)-the ratio of the open phase to a complete cycle of vocal fold vibration [26]. In a different view, creakiness has been regarded not as a region on a continuum between a fully open and closed glottis, but as the combined effect of different glottal mechanisms to control airflow through the throat [27]. These mechanisms are selectively used by speakers to modulate their phonation; for example, creakiness is presumably produced by vocal fold adduction and abduction, and upward and forward sphincteric compression of the arytenoids. By additionally engaging ventricular incursion, harsh speech may be produced. Different articulation settings translate into different acoustic features. E.g., for low OQ (when vocal folds remain closed longer than in modal phonation), the amplitude of F 0 relative to the next harmonic decreases. As a consequence, various metrics have been used to derive the presence of creakiness from acoustic signals including spectral tilt, F 0, F 0 jitter, and Harmonic-to-Noise Ratio (HNR). Among spectral tilt measurements, the aforementioned amplitude difference between F 0 (the first harmonic) and the second harmonic is known as H1-H2, and its formant-corrected version as H1 * -H2 * . These differences have commonly been used to predict creakiness, and the latter has seemed to yield better correlations with observed creakiness across different language corpora [10], [28]- [30]. Other differences such as H2-H4, H1-A1, (A1 being the tallest harmonic amplitude within the first formant), or peaks in higher formants (An) have been explored, but they did not outperform H1 * -H2 * in general, even after compensating for the influence of formant energies [31]. Computing a linear regression between quefrency and cespstral magnitude has revealed larger deviations (prominences) from this line for periodic signals compared to those observed on aperiodic signals [32]. Low Cepstral Peak Prominence (CPP) compared to that of modal phonation has also been associated with the presence of creakiness, arguably because of irregularities in the vibration of the vocal folds [33]. These irregularities have also manifested in high F 0 jitter, low HNR, and high SHR-Sub-harmonic to Harmonic Ratio (the ratio between the magnitude of harmonics below the fundamental frequency and those above it) [34]. Different articulation settings are the basis for subclassifications of creakiness. Depending upon the predominant articulation within a group of people, the performance of acoustic features as predictors of creaky segments varies. This variation in performance has hindered study and comparison of creakiness in speech since researchers have often chosen different acoustic features to discuss their findings. Recently, tools merging different acoustic features and heuristics have been developed for automatic prediction of creakiness. These tools are, in general, more robust than predictions made by individual acoustic features, and some of them are discussed in following. High speed video [35], electroglottographic recordings [36], and other methods have been proposed to detect creakiness, but audio recordings are perhaps the most popular source for creakiness detection. A brief selection of the methods to estimate creakiness from audio recordings is presented here. 1) Vishnubhotla's Method: Vishnubhotla and Espy-Wilson [37] proposed an extension to an aperiodicity, periodicity, and pitch detector. This extension was capable of detecting creakiness in running speech with no prior information about vocalic segments in the speech signal. To achieve that, the signal is split into frequency channels via a filter-bank, and periodic structures are sought in each channel. With this information, vocalic segments are determined, and creakiness within a vocalic segment is detected through a characterization of each frame based on a number of features. The authors claimed an 87% 2) Ishi's Method: Creakiness introduces long fundamental periods, longer than the window-size commonly used in F 0 analyses, yielding them ineffective. Ishi et al. [38] tackled this problem by performing a pulse-synchronized analysis of the signal. Additionally, they proposed to filter the signal between 0.1-1.5 kHz, select possible creaky frames considering local power peaks, and determine frame creakiness based on intraframe periodicity and inter-pulse similarity. They reported a 74% correct detection of creakiness with this method. 3) Kane's Method: More recently, an automatic detection of creakiness based on the presence of secondary excitation peaks in the residual signal of a linear prediction filter and residual peak prominence was proposed by Kane et al. [39]. In this method, both features are used as input of a decision tree classifier. They also used a routine to improve creakiness detection by excluding unvoiced and non-speech intervals, and by considering the duration of the creaky segments, etc. The authors reported that their method achieved significant improvements on the classification accuracy of creaky segments in clean recordings (measured as F-score) relative to that obtained with Ishi's derived methods. 4) Covarep Classifier: In addition to the acoustic features discussed by Kane et al. [39], the same authors acknowledged in [40] the importance of speech features such as H1-H2 in the characterization of creaky voice. They included these features along with those proposed by Ishi et al. [38] in an automatic detector of creaky voice. An implementation of this method is included in Covarep (v. 1.4.2) [41], a Matlab [42] library comprising several routines for speech analysis. In Covarep's implementation, an Artificial Neural Network (ANN) is used for the creakiness decision. This ANN is fed with 36 features: 12 basic features summarized in Table I, along with their first-and second-order time derivatives. F 0 and F 0 mean are computed with a method based on the Summation of Residual Harmonics (SRH). 5) Mori's Method: Mori et al. [43] analyzed spontaneous Japanese speech. Vowels were marked as 'creaky,' 'breathy,' or 'modal' by two experts. They considered 15 basic features (intensity, F 0, F 1, F 2, F 3, F 0 jitter, HNR, H1-H2, H1-A1, among others). Additionally, a 364-feature vector was extracted with openSMILE [44]. These features were fed to two machine learning algorithms: Random Forest-RF and Support Vector Machine-SVM. Using the mix of basic and extra features, they obtained an Area Under the Receiver Operating Characteristic Curve AUC = .903 for breathy voice, and an AUC = .872 for creaky voice with the RF algorithm. Although the AUC was high, the F-score was 0.62 for breathy and 0.59 for creaky voice, indicating that the RF performance was not as good as that achieved by experts. From the surveyed literature, it seems that the Covarep classifier is the most popular (see for example, [45]- [47]). This classifier outputs creakiness probability and binary decision per frame, every 10 ms. However, there is no consensus on how to determine the creakiness of a segment based upon the creakiness of its constituent frames. First of all, the threshold used in Covarep on the binary decision of a frame's creakiness seems to be corpus-dependent [48]. Hence, gauging the creakiness of a segment becomes problematic. Kuang [49] used the mean of frame binary decisions to determine a segment's creakiness, while other alternatives exist, such as computing the log of creaky probability for each segment and computing their mean, etc. [50]. Furthermore, many languages are known to have creaky phonation focused at specific times in vocalic segments. For example in Burmese, it has been reported that creakiness usually appears only in the second half of a creaky tone [68]. Thus, means taken from the entire vocalic segment may not be appropriate in these cases. Roughness is a psychoacoustic attribute of a sound (not only speech) comparable to pitch, loudness, sharpness, etc. In the same way that any sound has some degree of loudness or pitch, it also has some degree of roughness. Roughness produces continuous and quantitative changes associated with rapid amplitude modulations (between 15-300 Hz). Perceived roughness reaches a maximum for modulation frequencies around 70 Hz [51]. One asper (the unit of roughness) is defined as the roughness elicited by a 100% Amplitude-Modulated (AM) 1 kHz sinusoid at a modulation frequency of 70 Hz, presented at 60 dB (SPL) [52]. By manipulating the modulation index of this AM sinusoid, the absolute threshold of roughness perception was found to be 0.07 aspers, and its just noticeable difference ΔR/R = 17%. I.e., in order to perceive a change of roughness ΔR, it must be at least 17% of its current value R [51]. Apart from the modulation index and modulation frequency in AM sinusoids, roughness is influenced by the Sound Pressure Level (SPL), frequency deviation (in frequency modulated sinusoids), etc. [52]. Roughness is not exclusive of periodic amplitude modulations. Random modulations (like those found in narrowband noise) yield high values of roughness as well. Important contributions to the understanding of roughness came originally from Helmholtz [14], who observed that musical consonance could be explained in terms of the roughness produced by the interactions between frequency components of Fig. 1. Roughness dependencies: discrepancy between the temporal masking depth ΔL and the crest-trough difference h appears for rapid modulation frequencies. This difference varies with modulation period τ mod , the inverse of the modulation frequency f mod . simultaneous complex waves. Later, Terhardt [51] found that relative amplitude fluctuation almost exclusively explained roughness of amplitude-and frequency-modulated sounds, while Plomp and Levelt [53] linked the maximum roughness elicitation to a separation of ∼25% of a critical bandwidth (in terms of critical bands as reported by Zwicker et al. [54]). More recently, Pressnitzer and McAdams [55] found that phase and temporal asymmetries of a sound wave also contribute to the perception of roughness. Roughness seems to play a major role in sensory pleasantness [56], sound quality [57], musical dissonance [58], [59], psychoacoustic annoyance [52], and speech intelligibility [60]. Roughness prediction models can be categorized according to two views: spectral approaches, which are based exclusively on the spectral segregation made by the basilar membrane [61], and temporal approaches, which also take into account temporal aspects of the signal such as the phase-lock of nerve cells to the period of the stimuli [62], [63]. Among the former group, some of the most important models correspond to those of Helmholtz [14], Plomp and Levelt [53], Sethares [64], and Vassilakis [65]. In domains where the beating of harmonics is supposed to be the most important source of roughness (e.g., musical dissonance), spectral approaches have been widely adopted. Temporal approaches have more commonly been used in other applications such as measurement or prediction of auditory annoyance, etc. Creaky speech features temporal envelope modulations of around 20-70 Hz. Hence, in this study we opted for using a temporal model to predict roughness. In this kind of model, the roughness R is considered to be dependent chiefly on frequency and temporal resolution of the hearing system [52]. This is approximated as where f mod is the frequency of amplitude modulation and ΔL is the temporal masking depth, as illustrated in Fig. 1. ΔL accounts for the fact that rapid changes in the amplitude envelope of a signal are not accurately perceived, i.e., ΔL < h, the true cresttrough difference of the modulation. More precise prediction models of roughness compute the temporal masking depth through different auditory channels and different models vary upon the computation of ΔL E , the temporal masking depth of a given auditory channel. Von Aures [66] estimated global roughness R by computing specific roughness in 24 disjoint auditory channels corresponding to the bands of the Bark scale, as shown in Eq. ( 2). He used cross-correlation between adjacent bands to diminish the effect of random-like noise on the reported values of roughness. Daniel and Weber [15] optimized von Aures' model mainly by increasing (and overlapping) the number of auditory channels. In the current study, we measured objective roughness using a Matlab implementation of Daniel and Weber's model [67]. This implementation was found to closely match empirical results reported by von Aures [66] on the roughness of AM sinusoids at different modulation frequencies and different frequency bands (Pearson's product-moment correlation r = .971, p <.001). For the computation of roughness temporal profiles, we used frames that were 50 ms long, Blackman-windowed, and 80% overlapped (i.e., a 10 ms hop between adjacent analysis frames). These frames were divided into 47 auditory channels (one bark width with a half-a-bark overlap between channels), covering the audible spectrum from 20 Hz to 15.5 kHz. Specific roughness for each channel was computed and total roughness of each frame was obtained as a weighted sum of each channel's specific roughness. In this section, prediction of creakiness by roughness is detailed. First, we describe the nature of the Burmese language corpus used here. Next, we compare roughness, spectral tilt, and Covarep prediction temporal profiles, to illustrate the feasibility of roughness as a predictor. Finally, we present a series of experiments with Recurrent artificial Neural Networks (RNNs). A. Materials 1) Burmese: Burmese has four contrastive tones that are associated with vowels: Creaky, checked, high, and low. Every vowel has one of these four tones with the exception of vowels in minor syllables, not discussed here. Tone is contrastive, meaning that it is important in distinguishing words from one another. Table II illustrates this with four words that differ only in their tone. Tone in languages is commonly associated with the contrastive use of different F 0 contours; however, in Burmese, not only F 0, but also duration, intensity, and phonation are involved in the four-way tone contrast, as shown in Table II. Creaky tone involves a short-duration vowel with a high falling F 0 contour characterized by gradually increasing creakiness, especially in the latter part of the vowel. Checked tone differs from creaky tone in that it has an even shorter duration and a more abrupt final laryngeal constriction (a glottal stop coda), usually accompanied with some creak. In fact, checked tones are often viewed as a distinct syllable type, rather than a tone due to this final glottal stop. While checked tones are all spoken with glottal stop codas, the Burmese writing system encodes for obstruent codas with different places of articulation. Historically, these codas could vary in place of articulation, but this distinction has reportedly been neutralized in the modern spoken language [68]. High tone is described as having a long duration and sometimes as breathy. Low tone is described as having a similarly long duration but with modal phonation. While low tone has a low F 0 throughout its duration, high tone has a relatively higher peak F 0 with a contour that varies depending on its context; in citation form, it has a falling contour. High tone has a more moderate F 0 relative to creaky and checked tones, i.e., it is closer to a mid tone and the use of 'high' is to contrast it with the low tone. Regarding intensity, checked tone has the highest intensity, followed in order by creaky, high, and low tone. Previous acoustic studies of phonation in Burmese tones have met with varying levels of success. Spectral tilt (measured as H1 * -H2 * or similar spectral differences) showed a poor ability to predict creakiness [69]. Gruber noted that this result may have partly been due to the fact that creakiness is located towards the latter part of the vowel in Burmese, and may not be detectable at the midpoint, where these previous studies attempted to measure it [68]. He also found that measuring spectral tilt closer to the endpoint of the vowel yields a higher correlation with creaky tone status. 2) Corpus: The corpus used in our experiments comprised single words uttered by twelve native speakers (six of each gender) of mostly Yangon Burmese. One of the speakers (BRM510) was from Magway, Myanmar. Gruber [68] found that the phonation contrast in Burmese is neutralized in carrier sentences and is only seen in citation form (i.e., words read in isolation). We elicited the same word list in citation and carrier sentences, verifying Gruber's finding that creakiness is only produced in citation form. Thus, we focused only on the citation form for this study. All recordings were made in a quiet room where speakers wore a head-mounted unidirectional microphone (Shure WD30) connected to a solid-state recorder (Marantz PMD661 MKII) which stored the audio at a sampling rate of 44.1 kHz. 78 monosyllabic Burmese words repeated five times were produced in isolation by each speaker, resulting in a corpus of 4,679 tokens (one token was discarded). The word list was nearly balanced across the four language tones: 18 words had either creaky, low, or high tone, and 24 words had a checked tone. Examples of these tones uttered by the same female speaker are presented in Fig. 2; additional multimedia examples can be found at http://onkyo.u-aizu.ac.jp/software/creakbyr. In addition, the word list was balanced for coda type (no coda vs. nasal coda) and vowel quality ([i], [u], and [a]). We varied the orthographic obstruent coda, just in case this did have an effect on the spoken forms. Onset consonants were mostly alveolar (62 words) with some velars (15 words) and a single word with a palatal onset. Most words had obstruent onsets (59 words), 19 words had sonorant onsets (eleven with liquid [l] and eight with nasal onsets). Bilabial onsets were not used. The vocalic segments of the audio files (one per token) were manually labeled in Praat [70]. Onset of regular glottal pulses was used to mark the beginning of each vocalic segment. The offset of the vocalic segments was placed at the end of the final glottal pulse. Finally, all boundaries were moved to the nearest zero-crossing. 3) Validation: To assess whether the pronunciation of our speakers was similar to that described by the dictionary, we randomly selected a number of utterances per tone and speaker for manual verification. This review was independently performed by three of the authors, and we considered the spectrogram, waveform, and audio of each utterance to determine whether it was creaky or not. In total, the judges reviewed 492 utterances (about 11% of the corpus). The sample size for each combination of speaker and tone was ten, except in the case of checked tone (the most numerous among the four tones) where it was eleven. With this sample size, the margin of error was m = ±3 [t(9) = 2.262] with a confidence interval CI = 95%, a finite population correction of 0.889, and assuming a standard deviation SD = 0.05. Fleiss' Kappa index κ was computed to assess the inter-rater agreement with the library irr [71] in R [72]. It was found that there was very good agreement between the three judges, κ = .94 [z = 36.1, p <.001]. Fig. 3 presents the results of the manual verification: Percentages were computed from the ratio of the number of utterances rated as creaky by the judges and the total number of ratings per speaker and tone. The best agreements between dictionary entries and expert judgements were obtained for non-creaky tones (i.e., high and low), with a discrepancy of 2.3%, while for creaky tones (i.e., checked and creaky), these discrepancies amounted to 8.3%. Discrepancies between dictionary entries and expert judgements were more abundant for speakers BRM509 and BRM510. Besides their creakiness opinions, judges were also asked to comment on each utterance. From these comments, it seems that speaker BRM509 was some times exaggerating her pronunciation to the point where it was difficult to determine which tone was being used. On the other hand, BRM510 did not always use creakiness as expected: For checked and creaky Fig. 2. Examples of the vocalic segments for the four Burmese words presented in Table II produced by a female speaker (BRM506). Spectrograms on top, waveforms and roughness temporal profiles on bottom. Creaky frames, as determined by the roughness-based prediction, are indicated by filled squares. tones, he used short vocalic segments with a sharp fall of F 0, but without creakiness in many cases. This speaker came from a different region than the rest of the speakers, so it is possible that dialectal variation could explain his pronunciation. In all cases, the discrepancies found between dictionary entries and expert judgements could be considered normal in the context of inter-speaker or dialectal variation. We decided to preserve all utterances, which makes our corpus consequently noisy. We compared two common alternatives to estimate creakiness with a roughness-based prediction on the vocalic segments of the Burmese corpus as a way to assess the feasibility of the latter. These alternatives were spectral tilt and the method implemented in Covarep (creakiness probability and binary decision). 1) Alternative Methods: Spectral tilt was measured as H1 * -H2 * . For each utterance in our corpus, spectral tilt was measured every 10 ms with Voicesauce (v. 1.36) [73]. Covarep measurements were computed at the same rate with no modifications to the algorithm. We registered the probability of creakiness and binary decisions for creakiness as output by the program. 2) Creakiness Prediction Based on Objective Roughness: For the roughness-based prediction of creakiness, we used the roughness implementation described above in Section II-F. Additionally, the following adjustments were made: Monophonic audio recordings were resampled at 16 kHz to minimize the effect of high frequency roughness. Note that similar resampling is performed in Covarep and other methods, arguably to reduce computational load. Recordings were also DC-filtered, and amplitude peak-normalized to be 0 dB (re. Full Scale-FS). Since the actual pressure level at which each utterance was produced is unknown, it was assumed that each audio frame was produced at a sound pressure level of 80 dB, effectively eliminating SPL differences between frames. With these settings, preliminary visual inspection of roughness temporal profiles suggested that the vocalic segment of creaky tokens displayed frames with either large values of Fig. 3. Percentage of utterances rated 'creaky' by three experts for each tone in a sample of words randomly selected from our corpus. Top and bottom rows correspond to female and male speakers, respectively. roughness or extreme roughness changes from frame to frame. In light of this, we arbitrarily set an absolute roughness threshold of 4.0 aspers, above which a given frame was considered creaky. In similar fashion, a frame was considered creaky if it was 1 asper higher than any of those in a 5-frame vicinity (frames were 50 ms long, overlapped 80%). This was implemented via a Hankel-like matrix with five columns and as many rows as needed depending on the length of the vocalic segment. Roughness traces with estimated creaky frames in the vocalic segments of the words presented in Table II, as uttered by a female speaker, are shown in Fig. 2. 3) Results: Vocalic segment traces corresponding to the same tone, speaker, and feature (Covarep's probability and binary decision; spectral tilt; and roughness binary decision) were first time-normalized (i.e., setting the time to zero at the beginning of each vocalic segment, and dividing each frame time within it by the length of the vocalic segment) and then used to compute smooth conditional means. The smoothing was done with a generalized additive model using cubic splines and a span factor of 0.1% via the function 'geom_smooth' [74] in R. 95% confidence intervals were also computed and are shown in gray around the smooth lines in Fig. 4. As shown in Fig. 4(a) and 4(b), predictions made with Covarep were not always accurate, regardless of the output used (probability or binary). For our corpus, the two Covarep outputs produced very similar results: High and low tones yielded greater creaky frame probabilities (or number of creaky frames for the binary decision) than checked and creaky tones for some speakers (e.g., BRM505, BRM508). In other instances, differences between the traces were minimal (e.g., for speakers BRM503, BRM506, and BRM509). In fewer instances, Covarep results were in agreement with the tones (e.g., for speakers BRM501, BRM512). The results obtained with spectral tilt measurements were somewhat better: Creaky tones had a lower spectral tilt than high and low tones. These differences were not always clear throughout the time course of each vocalic segment, making creakiness judgement time-dependent, as shown in Fig. 4(c) (see speakers BRM504, BRM507, and BRM510, for example). Except in the case of speaker BRM510, roughness-based classifications consistently yielded a number of creaky frames for checked and creaky tones that was higher than that of high and low tones. This was especially clear in the second half of the vocalic segment. As mentioned before, a manual revision of BRM510's production revealed that this speaker did not always produce creaky and checked tones with creaky phonation. In agreement with [68], the roughness-based detector shows that Burmese creaky and checked tones have late creakiness in words in isolation. Finally, this experiment confirmed the feasibility of a roughness-based predictor of creakiness, details of which are provided in the following section. As discussed in Section III-B, there was clear evidence that roughness contours could be good predictors of creakiness. In that section, the criteria for determining the binary creakiness of a frame was based on visual inspection of roughness contours. It is very likely that we missed some patterns in the profiles that could improve the creakiness prediction made with psychoacoustic roughness. For that reason, we decided to experiment with Recurrent Neural Networks (RNNs) as a binary creaky segment classifier. The task given to the classifier was simply to decide whether a given vocalic segment is creaky or not. As previously mentioned, the length of the vocalic segment is in general different for each utterance. The roughness contour is a sequence of values (in aspers), one for each speech frame, and RNNs are especially well suited for modeling such kinds of temporal data series. The dataset for this experiment consisted of voiced segments from all 12 speakers' utterances. In order to get speaker independent results, we used the leave-one-speaker-out strategy, or 12-fold cross validation where the test data for each fold came from different speakers. The data from the remaining 11 speakers were randomly split into training and validation sets with a 10:1 ratio. Validation sets were used to tune some of the RNN hyperparameters, such as batch size, optimizer, learning rate, etc. We experimented with various RNN structures and hyper-parameter combinations, but a simple bi-directional RNN with a few layers and several dozens of Gated Recurrent Units (GRUs) turned out to be the most suitable. The RNN input is a one-dimensional roughness contour and the output is also a one-dimensional sigmoid node for binary prediction. The results given in all of the following tables are obtained with batch size bs = 10, Adam optimizer with learning rate lr = 0.001, Binary Cross-Entropy (BCE) loss function, and a maximum of 50 training epochs. The validation data loss was monitored during training and the model of the epoch with the smallest loss was saved for evaluation with the test data. In Table III, we summarize classification accuracy, precision, and recall results in terms of mean and standard deviation (SD) of the 12 folds. The best accuracy result of 94.5% ± 3.2% was achieved with a 2-layer RNN and 32 GRU nodes per layer. 2) RNN Trained on Covarep Features: As mentioned earlier, the Covarep toolkit can predict the probability of creakiness of a single frame from the 12 features shown in Table I and their first-and second-order time derivatives. To obtain segment-level decisions using Covarep predictions, we used majority rule voting or accumulated log probability score. Unfortunately, in both cases the classification results were close to random, i.e. about 50%. Apparently, the reason is that the Multi-Layer Perceptron (MLP) used in the toolkit had been trained on quite different data. To perform a fair comparison between Covarep and our approach, we trained a similar RNN using the same 12 features calculated from our data for the same voiced segments. The results we obtained are shown in Table IV. As can be seen, training an RNN with Covarep features yielded slightly better results than those obtained with only roughness contours. However, there are two main differences to take into account: i) roughness contours are unidimensional features (computed from several channels), while Covarep uses 12 disparate features, i.e.,  and their combination, resulting in 3-, 36-, and 39-dimensional vectors. Since RNNs are very good at modeling temporal dependencies, as we expected, there was no noticeable change in the performance with respect to the cases without derivatives. The combined results of the experiments discussed in Sections III-B and III-C suggest that models used to predict psychoacoustic roughness could also be used as predictors of creaky episodes in speech. Contrasting with the results of a roughness-based classifier, the creakiness detection routine implemented in Covarep failed in several cases to distinguish between creaky and non-creaky tones in our corpus, as shown in Fig. 4. These results persisted regardless of the Covarep's output used. When using RNNs, the performance of the roughness-based classifier was very similar to that achieved by using the same input as the Covarep predictor. However, we believe that our approach has several advantages with respect to other methods: 1) it uses 1-dimensional data for the prediction, as opposed to multi-dimensional data; 2) having a unique well-defined unit (i.e., asper) eases the comparison of creakiness among different studies, corpora, voices, etc.; perhaps more importantly, 3) psychoacoustic roughness is a perceptual feature, and we argue that it is more related to the phonemic classification made by listeners than other acoustic features: There seems to be several ways in which speakers can produce creaky voice. These articulation variations correlate differently with different acoustic features, such as H1-H2, CPP, etc. Regardless of how it is produced, creakiness is perceived, nonetheless, under a seemingly single category [12]. Therefore, focusing on later stages in the speech chain (i.e., in the auditory process) may be appropriate to describe and study phonation and phonemic contrast. We hypothesize that the psychoacoustic roughness prediction of creakiness works best when creakiness in speech is manifested as amplitude modulation (damped pulses, etc.) in vocalic segments, since that is what the psychoacoustic roughness model uses for its predictions. These modulations need not be periodic, so a roughness-based predictor should perform similarly for both creaky and rough speech, the latter as understood in the context of the CAPE-V framework-that is, \"perceived irregularity in the voicing source.\" Having a single dimension, a creakiness predictor based on psychoacoustic roughness, needs no ablation studies since all the output variance can be attributed to its single feature. For multi-dimensional predictors, on the other hand, ablation studies become indispensable to gain insights on the impact or relative weight that a single feature or set of features may have on the output. Psychoacoustic roughness increases with pressure level, so that \"for an increase in sound pressure level by 40 dB roughness increases by a factor of about 3\" [52, p. 260]. In our case, we assumed that each frame in the analysis was produced at 80 dB (SPL). Setting the frame SPL to a different value (or making it relative to the maximum amplitude of the signal) would produce changes in the roughness traces as well. For roughness-based classifications not using RNNs, it would be necessary to adjust the thresholds to determine the creakiness of a frame; for RNN implementations, retraining the network would also be necessary. Note that perturbations on phonation such as F 0-jitter, amplitude shimmer, etc., tend to increase when speech is produced at lower sound pressure levels [76]. Equalizing the frame intensities increases the measured roughness at quiet frames, such as those found towards the end of the vocalic part of our corpus (see Figs. The roughness model used in this research could predict with fair accuracy the presence of creakiness in speech. This model however, has been amended since originally proposed to account for the size of the auditory filters, i.e., replacing the Bark scale with narrower Equivalent Rectangular Bands-ERBs [77]; the effect of phase (such as those observed by the elicited roughness of a reverse saw-tooth signal) [55]; etc. We would expect that more sophisticated roughness prediction models yield more accurate classifications of creaky voice. Throughout our research, we relied upon classifications of creaky tokens from dictionary entries of the Burmese language. We manually inspected a sample of words and found that their actual production was, in general, the intended one. Performing an exhaustive review of all tokens and including other corpora may help to improve the accuracy achieved by the proposed classifier. Confirming the robustness of our roughness-based predictor is an ongoing project, including releasing freely available routines for creakiness prediction based on psychoacoustic roughness. It was possible to adequately predict creaky episodes in vocalic segments of speech using a psychoacoustic roughness model. A predictive model based on a Recurrent Neural Network using only psychoacoustic roughness yielded results comparable to those obtained with RNNs trained with higher dimensional data. However, the roughness-based approach yielded lower standard deviation, suggesting robustness to speaker variation. Additionally, no apparent advantage of augmenting the training data by time derivative features was found, and likewise, including roughness along with the acoustic features used in the multidimensional RNN did not improve prediction performance significantly. Authorized licensed use limited to: Queen Mary University of London. Downloaded on March 23,2025 at 00:33:29 UTC from IEEE Xplore. Restrictions apply.",
  "filePath": "C:\\Users\\Alex Isaev\\Documents\\truth source\\citation-verifier\\src\\document-database\\documents\\prediction_of_creaky_speech_by_recurrent_neural_networks_using_psychoacoustic_ro.json",
  "sourcePdf": "C:\\Users\\Alex Isaev\\Documents\\truth source\\citation-verifier\\src\\document-database\\pdf-documents\\21.pdf",
  "doi": "10.1109/JSTSP.2019.2949422",
  "year": "",
  "journal": ""
}