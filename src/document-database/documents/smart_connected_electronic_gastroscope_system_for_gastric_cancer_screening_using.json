{
  "id": "smart_connected_electronic_gastroscope_system_for_gastric_cancer_screening_using",
  "title": "Smart connected electronic gastroscope system for gastric cancer screening using multi-column convolutional neural networks",
  "authors": [
    "HaoWang",
    "ShuaiDing",
    "DeshengWu",
    "YoutaoZhang",
    "ShanlinYang"
  ],
  "content": "Gastric cancer is a major threat to public health in China. In 2016, the incidence of gastric cancer in China accounted for 47% of the total cases of gastric cancer worldwide. That is approximately 2000 people are diagnosed with gastric cancer every day. Fortunately, clinical studies show that screening and early diagnosis of gastric cancer via endoscopy system are significantly helpful for therapy (Siegel, Miller, and Jemal 2016). Ninety per cent of patients with early gastric cancer have their life expectancy extended by more than five years through a series of surgeries, such as endoscopic mucosal stripping and endoscopic mucosal resectioning (Pellise et al. 2017). While gastric cancer screening rates of potential gastric cancer patients are approximately 50% in Japan and Korea, the rate is lower than 15% in China (Chen et al. 2016). The recent advances in endoscopy led to the development of novel gastric cancer detection methods, such as endoscopic ultrasonography (EUS) (Pyo et al. 2016) and endoscopic submucosal dissection (ESD) (Nagar 2017). However, the advances in gastroscopy equipment and computer aided diagnosis techniques have not substantially mitigated the global prevalence of gastric cancer. For example, while most hospitals in major cities in China have the capacity to perform gastroscopy, the mortality rates from gastric cancer are still approximately 15% in such cities. The high mortality rates with endoscopy are mainly due to the three reasons: (1) inadequate examination of gastroscopy results; (2) lacking the ability to identify and detect the gastric lesions, i.e. miss-diagnosis; or (3) inaccurate sampling when biopsying pathological tissue from the lesions (Le Clercq et al. 2015). According to recent studies (Kang and Qiao 2014) and (Sugano 2015), the miss-diagnosis rate for gastroscopists with less than 10-year working experiences is approximately 25%. In addition, it is almost inevitable even for experienced gastroscopists in China to run into miss-diagnosis or misdiagnose, due to their heavy medical image analysis load, i.e. approximately 50 patients per gastroscopist per day. The recent efforts have focused on designing high-end medical systems to assist gastroscopy, in particular, adopting machine learning techniques to enhance both inspection and diagnosis (Priya and Ranjith Kumar 2015). Such an approach is attractive not only for academia but also for medical industry that manufactures gastroscopy equipment. For example, the system jointly developed by London Royal Free National Health Service and DeepMind can achieve better screening than experienced doctors in analysing fundus images (Gulshan et al. 2016). Ding et al. proposed a disease prediction system that integrates Clustering, Association analysis and Collaborative filtering techniques for better diseases prediction (Ding et al. 2017). Demir et al. developed a decision support toolkit to assist the treatment of Parkinson's disease (Demir et al. 2015). While these systems effectively enhance medical image analysis, they face a major challenge in gastroscopy: How to perform intelligent screening of gastric cancer during gastroscopic examination. To address this challenge, this paper proposes SCEG, a smart connected electronic gastroscopy system that performs dynamic cancer screening in gastroscopy, which helps prevent miss-diagnosis or misdiagnosis in real-time. SCEG consists of four main modules: electronic gastroscopy sub-system, all-in-one integrated sub-system, endoscopic video management sub-system and cloudbased screening service sub-system. Given that ensemble model outperforms other single classifiers in image analysis, this paper develops a novel AdaBoost-based multi-column convolutional neural network (MCNN) to assist gastric cancer screening. The remainder of this paper is organised as follows: Section 2 briefly reviews the relevant literature. Section 3 elaborates the architecture of the proposed SCEG system and its four modules in detail. Section 4 presents our gastric cancer screening approach, with the experiments and comparison results summarised in Section 5. Section 6 concludes the paper and discusses the future work. Gastroscopy is a procedure that has been performed for more than 140 years. Its operating principle is to place a hose into the human oesophagus to acquire images under illumination from a cold light source. Nowadays, fibre gastroscopes and electronic gastroscopes are the two main types of gastroscopes adopted in the clinic. A fibre gastroscope consists of an optical fibre and a bright light source that is located outside of the imaging system. An electronic gastroscope consists of a miniature camera that is installed on the front end of an endoscope to obtain digital images in real time. The acquired images can be viewed on television monitors, thereby allowing the doctor to interactively focus on lesion detection (Ye et al. 2015;Chon and Kim 2017). Performing gastroscopic examinations regularly can effectively improve the detection rate and reduce the mortality rate of gastric cancer. Artificial intelligence-assisted medical image recognition is an emerging auxiliary diagnostic method. Cuingnet adopted SVM (supporting vector machine) to detect the diffusion alterations associated with stroke outcome (Cuingnet et al. 2011). Kannan proposed a method based on robust kernel FCM (fuzzy C-means clustering) for the segmentation of breast medical images (Kannan et al. 2011). With the recent breakthrough in deep learning, many deep learning-based medical image recognition systems have been developed. Santosh adopted multilayer perception neural networks and random forest to enable automated chest X-ray screening (Santosh and Antani 2017). CNN (convolutional neural network)-based methods can achieve excellent experimental results in multiple medical image analysis fields. For example, Microbiana et al. reported 85.5% accuracy when analysing a database of lung CT image slices (Dou et al. 2016). As such, enhancing high-end medical equipment with artificial intelligence technology (e.g. CNN) is a promising field not only to equipment providers but also computer scientists for the coming decade. Multi-column convolutional neural network (MCNN) is an architecture developed to address the single-image crowd counting problem in CNN, which can improve the prediction performance on large-scale data-sets (Zhang et al. 2016). The final prediction in MCNN is generated by averaging the single predictions of all trained neural networks. In recent years, MCNN has been applied to image analysis using different ensemble strategies (Dong et al. 2015;Oh et al. 2017). Such models can be used to extract deeper characteristics or relationships through the combination of networks of different sizes. In this paper, we are to develop a novel AdaBoost-based MCNN approach to improve the prediction performance of cancer screening in electronic gastroscope system. Gastroscopy is a reliable method for early screening of gastric cancer in clinical practice. However, the detection rates are often less than 15% in developing countries. In this paper, we propose SCEG, a smart connected electronic gastroscopy system that performs dynamic cancer screening in gastroscopy. SCEG is designed to assist gastroscopists in detecting lesions during gastroscopic examinations, which helps reduce miss-diagnoses or misdiagnoses in real time. As shown in Figure 1, a SCEG system contains four main modules: the electronic gastroscope (EG), the all-in-one integrated system (IOS), the endoscopic video management system (EVMS) and the cloud service system for gastric cancer screening (CSS). The EG module acquires optical signals. It is compatible with a typical electronic gastroscope through its buckletype design, which uses an external CCD camera and converts the collected light signals into electrical signals through its link with the IOS. The IOS module incorporates a cold light source, an acquisition card, a power supply system and a medical monitor. When the system is in operation, the electrical signals collected by the EG module are processed into digital signals by the IOS module's acquisition card. The processed results are then displayed on the medical monitor. The IOS provides a connection between the management system and the acquisition device (surgical camera or AR glasses). The EVMS module supports the retrieval, storage, communication and analysis of gastroscopic images associated with each patient. To assist gastric cancer screening, the CSS module can send the automated analysis result (i.e. highly suspicious or normal) to EVMS for each screening request. The gastric cancer screening model employed in the CSS module has a significant impact on the efficiency and performance of the SCEG system in clinic practice. We next elaborate the MCNN model in this paper. To enable flexible SCEG adaption, equipment manufacturers have the ability to integrate new approaches to address the deficiency appearing in practice. The intelligent gastric cancer screening model adopted in SCEG is based on MCNN (multi-column convolutional neural networks). By combining the results of multiple CNNs using the AdaBoost strategy, which is widely used in production research (van der Spoel, Amrit, and van Hillegersberg 2017), SCEG can automatically analyse lesions in gastroscopic images and detect gastric cancer in real time. To estimate the performance of the model for automated gastroscopic diagnosis, four clinical metrics (i.e. sensitivity, specificity, miss-diagnosis rate and misdiagnosis rate) have been applied in model training and generation. In this paper, we develop the AdaBoost-based MCNN model to integrate the classification capabilities of various CNNs and perform gastric cancer screening via cloud service, as shown in Figure 2. This approach consists of three phases: (1) Gastroscopic image preparation. This phase encompasses the raw data classification task and the data conversion task for the CNNs (i.e. AlexNet, GoogLeNet and VGGNet). ( 2) Model training and validation. In this phase, the prepared data-set is divided into a test set, a training set and a validation set to train each CNN and validate its performance. (3) Gastric cancer screening. The classification results of each CNN are migrated to the next column to focus on correcting erroneous results. Finally, the MCNN integrates the results of all trained CNNs to obtain the final prediction using the AdaBoost strategy. Approximately 90% of the data produced in a gastroscopic examination are images, and different images exhibit different characteristics. To achieve feature extraction using CNNs, we have to pre-process these images, which are typically stored in an endoscopic video management system (EVMS) and/or a picture archiving and communication system (PACS). Data pre-processing is divided into two tasks: (1) the raw data classification task. The original gastroscopic images are collated and prepared for CNN model training; images containing cancer lesions are tagged as 'highly suspicious', whereas images containing no cancer lesions are tagged as 'normal'. (2) The conversion task. To enable CNN training, the prepared data-set is divided into a training set, a test set and a validation set. The gastroscopic images are then converted into LMDB format (Zhu et al. 2008). The image analysis model is the core of the gastric cancer screening approach. In recent years, AlexNet (Krizhevsky, Sutskever, and Hinton 2012) and GoogLeNet (Szegedy et al. 2016) are widely employed in medical image understanding. VGGNet can be used to extract image features (Simonyan and Zisserman 2015) by repeatedly stacking small 3 × 3 convolution kernels and 2 × 2 pooling layers. Compared with other network models, VGGNet offers a better generalisation ability and can learn the characteristics of larger spaces (Simonyan and Zisserman 2015). Considering the fact that ensemble model outperforms other single classifiers in image analysis, we construct an MCNN model by combining various training CNN models (i.e. AlexNet, GoogLeNet and VGGNet) to enhance the performance of gastric cancer screening. To ensure the stability of the proposed model in the presence of variations in the training data-set (gastroscopic images), we perform data post-processing using a cross-validation approach with training-validation-test steps. The training and validation of a CNN model ends when a specified maximum number of iterations is reached or the test accuracy reaches 80% or better. Otherwise, the network parameters are adjusted through backpropagation. The network performance is gradually improved until one of the above two conditions is met. Step 1 Step 2 Step 3 The entire data-set is divided into 5 subsets, and the training sets consist of the possible combinations of 3 of these 5 subsets, which are constructed for cross-validation for a total of C where TA j represents the test accuracy of CNN model on j-th training set (1<=j<=6). Following the analysis model presented above, a training result is generated for each neural network, which includes a set of classes and the corresponding test accuracy of classification. Based on the pre-experimental gastroscopic image analysis performance of each CNN model, we employ the AdaBoost strategy to generate the MCNN model for gastric cancer screening. The structure of the AdaBoost-based MCNN is illustrated in Figure 3. Suppose h t x i ð Þ is the analysis result (highly suspicious or normal) of gastroscopic image x i predicted by CNN model t (t = 1 … 3), y i is the correct classification result, ε t is the ratio of the number of misclassified images to the total number of images; the weight of CNN model t is defined as: Note that when ε t > 0.5, the related CNN model should be discarded. To enhance the screening performance of the next column (i.e. CNN model) on miss-diagnosis or misdiagnosis gastroscopic images, we must update the weights of the erroneously classified elements in the overall data-set. Suppose D t x i ð Þ is the weight of misclassified image by CNN model t, the updated weight by CNN model t + 1 can be calculated by: The updated weights of miss-diagnosis or misdiagnosis gastroscopic images would be employed in training the next CNN model. In our approach, the weights are updated by utilising the OpenCV image extension method, which applies Gaussian blur (Liu et al. 2016), central distortion and translation by a distance of one-eighth of the width of the image itself. Finally, according to the AdaBoost strategy, the gastric cancer screening result for each gastroscopic image x can be calculated by: where a t is the weight of CNN model t, h t x ð Þ is the prediction result of gastroscopic image by CNN model t. Our proposed AdaBoost-based MCNN approach makes it possible to deal with various types of intelligent medical image analysis, e.g. fundus images, brain CT images and lung CT images. Given that the proposed AdaBoost-based MCNN approach is the most important component in the SCEG system, we next conduct experiments to evaluate its characteristics, sensitivity, miss-diagnosis rate and misdiagnosis rates for gastric cancer screening. We collected and pre-processed a large number of real gastroscopic images and performed comparative studies with three widely adopted approaches. We evaluated the model accuracy based on the actual diagnoses. The endoscopic data used in the experiments are from actual medical examinations. These data include the records of the first affiliated hospital of Anhui Medical University, which were collected over a total of 18 months from March 2016 to September 2017, with examination reports for a total of 25,659 cases. By collaborating with the gastroscopist to eliminate invalid data (e.g. partial reports that did not contain gastroscopic images or for which the stored gastroscopic images were corrupted), we extracted 13,108 suitable gastroscopic examination reports. There are a total of 1350 images depicting cancer (highly suspicious) and 103,514 normal images. Each image was resized to 512 × 512 pixels. Figure 4 shows the sample images for diagnosing gastric cancer. We built the training data-set following the train-validation-test pattern (Deng et al. 2009). In general, 60% of the original gastroscopic images were used as the training set, which helped train and adjust the parameters of the CNN models. Twenty per cent of the original images were used as the validation set, which helped tune the hyperparameters; and the remaining 20% of the images were used as the test set, which helped evaluate the effectiveness of the proposed approach. During training, the iterative network status was tracked based on the loss function of the CNN (Zhao et al. 2015). In particular, we made several adjustments to the neural networks used in the experiments. To compare the performance of each CNN model, we set the maximum number of iterations for each neural network to 450,000. Based on the processed image data-set, the final output of each model places the images into two categories: highly suspicious and normal. We followed the widely adopted metrics in medical diagnosis field to evaluate the experimental results. Suppose wl (respectively, nwl) is the number of images with (respectively, without) gastric cancer lesions, cwl (respectively, cnwl) is the number of correctly identified disease images (respectively, incorrectly identified images), the performance metrics are defined as: The CNNs were implemented using the Caffe framework (Jia et al. 2014). Each network batch size was set to 32 to avoid overfitting during training, and the integration of each column into the MCNN architecture was coded in Python. All experiments were performed on a machine running Linux OS with an Intel Xeon @ 2.16 GHz CPU, an NVIDIA GeForce Titan X 4-way graphics card and 128 GB of RAM. In this section, the experimental results are split into three parts. First, we present examples of the processing of images in our system. The results obtained using the fine-tuned CNNs models follow (i.e. AlexNet, GoogLeNet and VGGNet), and finally, the AdaBoost-based MCNN performance is compared with the fine-tuned CNN models and traditional nondeep learning classification methods. In our experiment, we fine-tuned the neural network parameters using the verification set, which is currently a well-accepted method in image recognition. Figure 5 shows the processing of gastroscopic images in our system. Figure 5(a) shows a gastroscopic image that exhibits characteristics of gastric cancer. With the application of CNNs, the dimension of the image is reduced via kernel convolution, which weakens the irrelevant feature areas. After processing the first convolution layer, the characteristics of noisy areas are diluted, as shown in Figure 5(b). To avoid loss of image information during processing, we use the activation function in the CNN to further distinguish feature areas from irrelevant regions of the image, as shown in Figure 5(c). Although the final gastroscopic images are no longer recognisable after multiple convolution and pooling layers, the relevant features of the images are not lost. We vectorise the gastroscopic images using the final fully connected layer and then classify the images via the final classifier (Figure 5(d)). To verify the CNN models combined in the AdaBoost-based MCNN, we used two indicators, test accuracy and loss, to describe the performance of CNN models during training. In general, a single CNN model whose accuracy is greater than 50% can be used for AdaBoost. The recorded results are shown in Figure 6. The transverse axis shows the number of training iterations for the neural network. The left vertical axis represents the loss function value, and the right vertical axis represents the test accuracy. With an increasing number of iterations, the test accuracy tends to reach a stable value, and the corresponding loss function values decline and converge to 0, demonstrating that the CNN models exhibit stable performance for gastric cancer screening. With increasing network complexity, the rate of decrease of loss gradually increases, which indicates that the CNNs that we have chosen can be used for the generation of AdaBoost-based MCNN model. In addition, we employed the approach in Section 4.4 to train and validation an AdaBoost-based MCNN model for gastric cancer screening. Firstly, the error rate for AlexNet model on our test data-set is ε 1 = 45.78%, and the weight of AlexNet model is a 1 ¼ 1 2 ln 1Àe 1 e 1 = 0.098. Secondly, we updated the weights of incorrectly screening images by utilising Equation (3) as 1Àe 1 e 1 = 1.21. This method is applied to 2634 erroneously classified images by AlexNet model. Figure 7 shows lesion-containing images after expansion using six methods, namely (from left to right and from top to bottom), Gaussian blur, central distortion and translational deformation in four directions. In this way, we increased the number of incorrectly classified images to 3161. Finally, we calculated the weights for GoogLeNet and VGGNet to be a 2 = 0.087 and a 3 = 0.299, respectively. After calculating the weights for each column (CNN model) in the MCNN, to further investigate the effectiveness of the proposed approach, we applied the analysis to the validation set and compared the experimental results obtained with the fine-tuned CNN models. The experimental results are compared in Table 1. From the table, we found that the competing CNN models exhibited limited diagnostic capabilities for the screening of gastric cancer. They behave consistently in terms of sensitivity and can only identify some gastroscopic images with disease features. We could also infer that the performance of the AdaBoost-based MCNN is superior to that of any single neural network in terms of each metric. The specificity and sensitivity are significantly increased, and the rates of miss-diagnosis and misdiagnosis are significantly decreased. As for the rates of miss-diagnosis, we found that as the depth of the neural network increases (from AlexNet to AdaBoost-based MCNN), the rate of miss-diagnosis gradually decreases. Additionally, to further support the benefits of deep learning in our method, we chose KNN (Ding et al. 2015) and SVM-based (Shen, Wu, and Suk 2017) classifiers for comparison. While SVM and KNN are high-efficiency methods for performing classification in medical image recognition, the comparison results in Table 2 showed that our proposed method is significantly better than SVM with K-means and KNN, especially for the miss-diagnoses rate, the most concerning indicator during the process of clinical diagnosis for gastric cancer screening. To further explore the performance of the system in actual use, we conducted several practical tests. We used gastroscopic images provided by the hospital, among which a 512 × 512-pixel endoscopic image occupies approximately 100 kB of storage space. In the SCEG system, the proposed method worked, but it is worthwhile to note that the time required to process a single gastroscopic image from the current sample set using the integrated learning method is approximately 2 s for a network bandwidth of 2 Mbps. At the same time, the performance of the system is also affected by the image quality of the CCD camera. When the bandwidth is less than 512 kbps, images cannot be transmitted stably. These findings demonstrate that the proposed approach can effectively prevent gastroscopists from miss-diagnosis or misdiagnosis of gastric cancer in gastroscopy. In practice, the new designed SCEG system makes it possible to deal with various types of gastric cancer screening (stages T0-T4) using the AdaBoost-based MCNN approach. Given that our system is evaluated using medical images with or without evidence of gastric cancer, its applicability for screening other illnesses is yet to be tested. Gastroscopic examinations are very important in the medical field to effectively identify lesions that are characteristic of stomach disease. Currently, the major challenges are that doctors have difficulty in locating lesion spots and that inexperienced doctors have high misdiagnosis rates. In this paper, we propose the SCEG system, an intelligent diagnosis and treatment system that combines artificial intelligence with electronic gastroscope. The analysis tools of the SCEG system are hosted in the cloud, and the AdaBoost-based MCNN is developed to enhance the performance of gastric cancer screening. With the increasing demand for medical images analysis, there is an increase in the number of hospitals that start to adopt smart connected medical equipment in clinical practice. By integrating the inspection equipment and the diagnostic equipment, SCEG provides a way to improve the ability of diagnosis in gastroscope room. The adoption of deep learning techniques can effectively improve lesion detection and cancer screening. In future work, we will build new CNNs and modify the integration strategy to improve the generalisation and analysis capabilities of the AdaBoost-based MCNN. It is also necessary to further integrate various types of artificial intelligence technologies to design high-end medical equipment so as to greatly improve the decision-making capacity for medical treatment. No potential conflict of interest was reported by the authors. H. Wang et al.",
  "filePath": "C:\\Users\\Alex Isaev\\Documents\\truth source\\citation-verifier\\src\\document-database\\documents\\smart_connected_electronic_gastroscope_system_for_gastric_cancer_screening_using.json",
  "sourcePdf": "C:\\Users\\Alex Isaev\\Documents\\truth source\\citation-verifier\\src\\document-database\\pdf-documents\\15.pdf",
  "doi": "10.1080/00207543.2018.1464232",
  "year": "",
  "journal": ""
}