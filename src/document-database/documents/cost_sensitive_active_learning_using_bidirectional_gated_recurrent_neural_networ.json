{
  "id": "cost_sensitive_active_learning_using_bidirectional_gated_recurrent_neural_networ",
  "title": "Cost sensitive active learning using bidirectional gated recurrent neural networks for imbalanced fault diagnosis",
  "authors": [
    "PengPeng",
    "WenjiaZhang",
    "YiZhang",
    "YanyanXu",
    "HongweiWang",
    "HemingZhang"
  ],
  "content": "It is crucial to ensure the reliability and stability of industrial processes as a slight mistake could precipitate a disaster. This has resulted in the development of a wide range of fault diagnosis methods over the past few decades, e.g. the model-based approaches and the data-driven techniques. Specifically, the former has the challenge of obtaining accurate mathematical models for complex processes while the latter has become increasingly popular due to the advancement of new technologies such as 5G communication, Internet of things (IoT) and AI. In the past decades, a number of data-driven fault diagnosis methods have been proposed, such as principal component analysis (PCA) [1] , support vector machine(SVM) [2] and deep convolutional neural network (DCNN) [3] , etc. Despite the effort to make the methods mentioned above more robust and reliable, they still may fail under three main circumstances. First, the process variables present auto-correlated and non-stationary behaviors. Second, faults tend to occur much less frequently so the normal classes turn to have more instances than the fault classes. Third, the data generated are very large nowadays with the rapid application of IT in industry, and as such it is often extremely hard to obtain enough labeled data. For the first circumstance, recurrent neural network(RNN) has shown good performance in a wide range of time-series problems, in addition to the employment of deep learning techniques for learning the dynamic information in an end-to-end manner. Gated recurrent unit is a variant of recurrent units, which proves to be much simpler to compute and have better generalization performance compared with the popular long short-term memory (LSTM) [4,5] unit. For the second circumstance, the standard supervised machine learning methods are challenged as these models pay much attention to the majority examples and distort the minority examples. Researchers from the machine learning community have developed some techniques to tackle the class imbalance problem including sampling methods, cost sensitive methods and kernel-based methods. Unfortunately, few of these methods take the class imbalance issue into consideration when they are applied to monitoring industrial processes. For the third circumstance, if an appropriate rule is adopted to select the unlabeled instances for annotating, only a very small number of samples need to be annotated and the performance of the classification model may be improved to a large extent. In the machine learning community, this is called active learning. Common strategies used in active learning are uncertainty sampling, margin sampling and entropy sampling. As to the more efficient use of unlabeled samples, some semi-supervised approaches and active learning methods are introduced to monitor complex industrial processes. However, the class imbalance issue cannot be well addressed when exploring the unlabeled data. To better take class imbalance into account when selecting the instances, cost sensitive active learning is developed by researchers from the machine learning community, e.g. the maximum expected cost and cost-weighted minimum margin strategies [6] . To address the challenges mentioned above, a cost sensitive active learning framework using the bidirectional gated neural recurrent unit is proposed for fault diagnosis of complex engineering processes. Specifically, the bidirectional gated recurrent unit (BGRU) [7] is proposed to capture dynamic information of a complex process; varied weights are assigned to samples to cope with imbalanced data classification; and cost sensitive active learning is proposed to explore the unlabeled data. The main contributions of this paper are summarized as follows: • Bidirectional gated recurrent unit (BGRU) is proposed to learn the dynamic information from process data. Compared to the conventional fault diagnosis methods, BGRU can learn a fault diagnosis model in an end-to-end manner with the consideration of time series. The rest of this paper is organized as follows. In Section 2 , the related work is discussed. In Section 3 , the bidirectional gated recurrent unit and cost sensitive active learning methods are introduced, and on this basis, the evaluation index is introduced. In Section 4 , the proposed cost sensitive active learning using a bidirectional gated recurrent unit (CSALBGRU) fault diagnosis framework is described in detail. In Section 5 , the effectiveness of the proposed CSALBGRU framework is evaluated by its application to both the Tennessee Eastman process (TE) dataset and a real plasma etching process dataset. The main conclusions of this work are summarized in Section 5 . The data-driven methods can be divided into two main classes. The first major class of data-driven methods involves the multivariate statistical methods, such as principal component analysis (PCA), independent principal analysis (ICA), partial least square(PLS) and linear discriminant analysis (LDA) [1,[8][9][10] . These methods emphasize the projection of raw data onto a lower-dimensional feature space where fault diagnosis can be performed. To address the dynamic properties of raw process data, some dynamic methods have also been proposed to use extended vectors that concatenate the current data with a certain number of past process data, e.g. the dynamic PCA(DPCA) [11] , the dynamic ICA(DICA) [12] , the dynamic PL S(DPL S) [13] , and the dynamic LDA(DLDA) [14] . The main disadvantage of these methods is that they may aggravate the \"curse of dimensionality\" problem if the number of process variables is considerably large. Another major class of data-driven methods involves the use of conventional machine learning techniques, such as support vector machine(SVM) and Fisher discriminant analysis (FDA) [2,15] . The major challenge for these methods is that they may fail when dealing with a relatively large amount of data. More recent development of the field has also seen the applications of new machine learning algorithms such as deep learning, sparse auto-encoder (SAE) [16] , deep convolutional neural network (DCNN) [3] and variational auto-encoder(VAE) [17,18] . Most of the current data-driven fault diagnosis methods assume that the number of normal samples and fault sample is about the same. However, the number of fault data is generally much smaller than the number of normal data in real industrial scenarios. This means that the performance of these traditional methods tends to be very poor under the imbalanced distribution -the diagnosis result of the normal condition may be accurate while the diagnosis result of the fault condition may be inaccurate. Hence, more and more researchers start paying attention to the class imbalance problems in fault diagnosis recently. For example, Li et al. propose to use the deep transfer learning to develop an effective diagnostic method with insufficient training data as the knowledge transferred from the sufficient additional datasets can help address the issue of insufficient training data [19] . Lin et al. add the Laplacian regularization term into the original objective function of Deep Auto-encoder (DAE) to formulate the deep Laplacian Auto-encoder(DlapAE) and investigate its application in imbalanced fault diagnosis. The experiment shows that DlapAE has a better generalization performance and is more suitable for feature learning of imbalanced data [20] . Li et al. propose to use the generative neural networks(GANs) to generate the fault samples to assist model training [21,22] . However, most imbalanced fault diagnosis methods focus on using data generation techniques, while little work has been done to take advantage of cost-sensitive learning algorithms and apply these algorithms in this particular field. Besides, the unlabeled process data can be easily collected while the acquisition of labeled data is very expensive in the industry from a practical point of view. How to select the informative samples and label them in the case of imbalanced data becomes an important problem. Most of the current literature is focused on the discussion of using active learning to fine tune the diagnosis model. For example, Jiang el at. use active learning to identify the most informative sensor data which can then be labeled for updating the learned parameters of deep neural network(DNN) [23] . Yin et al. incorporate active learning to fisher discriminant analysis (FDA) for industrial fault classification [24] . It is noteworthy that how to develop active learning under an imbalanced fault diagnosis situation is still an open problem and few research has tried to discuss this to the best of the authors' knowledge. Recurrent neural networks (RNNs) [5] have been successful applied to handling sequential data in various fields. As most of the industrial processes are inherently dynamic, it is natural to consider RNN as an alternative model. Given an input sequence x = (x 1 , . . . , x T ) , hidden vector sequence h = (h 1 , . . . , h T ) and output vector sequence y = (y 1 , . . . , y T ) can be derived using the following equations: where denotes the activation function, and the most popular activation function is usually an elementwise application of the sigmoid function. U denotes the input-hidden weight matrix, W denotes the hidden-hidden weight matrix, and b is the hidden bias vector. In Equation (2), V denotes the hidden-output weight matrix, and c is the output bias vector. It is hardly possible to capture the long-term dependencies of RNNs as the gradients tend to either vanish or explode. Hence some researchers have made efforts to design a more sophisticated activation function to solve these problems. For example, the long short-term memory (LSTM) [4,5] unit is first devised to capture the long-term dependencies. And more recently, another variant of recurrent unit, gated recurrent unit (GRU) [25] , is also proposed, which is much simpler to compute and has better generalization performance compared to the LSTM unit. The frameworks of the LSTM unit and the GRU unit are shown in Fig. 1 . For LSTM, it uses an output gate to control the amount of memory content exposure. where o t is the output gate computed by: where σ is the logistic function. The memory cell c t is maintained by removing (forgetting) some existing memories and adding some new memories: the new memories c t is : The extent to remove and add memories is controlled by the forget gate f t and the input gate i t . f t is computed by: and i t is computed by: where b denotes the corresponding bias vectors. Similar to the LSTM unit, gated recurrent unit (GRU) use gates to control the flow of information inside a unit, but it has no memory cells. The hidden states h t is a linear combination of previous hidden states h t-1 and new hidden states h t : where z t is the update gate which controls how much its new activation is updated. z t is computed by: The new activation h t is computed by: where r t is the forget gate, similar to the update unit in LSTM: While conventional RNNs only exploit the previous information, the bidirectional RNNs (BRNNs) [26] can process data in both directions, as shown in Fig. 2 . The output y of BRNN can be obtained by computing the forward hidden sequence -→ h t and backward sequence ←h t in an iterative manner, using the following equations: Combining BRNN with GRU gives BGRU, which can be used to access the long-term full sequential information of a given sequence in both directions. As a fault diagnosis problem can generally be viewed as a classification problem, cross entropy is adopted as the loss function. As to the sample weights, the weighted cross entropy is given by: where θ denotes the neural network parameters, N denotes the number of samples, M denotes number of faults, y i denotes the true label and ˆ y i denotes the predicted probability. In the training phase of BGRU, some tricks are adopted and described as follows: • Batch Normalization [27] . Batch normalization is proposed to solve the covariate shift problem. In the training phase, data normalization is done at the intermediate layer after feeding a batch. The mean and variance of the intermediate layer will be 0 and 1 after batch normalization. The benefit of using batch normalization is that they can improve the generalization performance and speed up the training process. • Drop Out [28] . Drop out is proposed by Hinton et.al to avoid the overfitting of neural networks. During the forward propagation of neural networks, neurons will not work with a probability of P . Multiple sub-networks are considered to be trained when adopting dropout. This will result in a better generalization performance of the trained network. • L1-norm Penalty [29] . For deep neural networks, smaller weights are encouraged since larger weights can lead to instability in the network. Usually, the L2-norm or L1-norm penalty in the loss function is preferred. In this paper, the L1-norm penalty is adopted to get smaller and sparser weights to avoid overfitting. Thus the loss function becomes: where α is the trade-off parameter. Cost sensitive active learning is proposed to explore a large amount of unlabeled data while taking the class imbalance issue into consideration. Active learning (AL) mainly consists of two stages, namely query and labeling. The most commonly used query strategy for AL is uncertainty sampling while minimum confidence and minimum margin are the two common uncertainty strategies. (1) Minimum confidence: this strategy selects the samples with the least confidence, that is to say, the strategy selects: arg min where D denotes the labeled dataset and D + denotes the unlabeled dataset and denotes the most probable class of an unlabeled sample x . (2) Minimum margin: this strategy selects the samples with the minimum difference of confidence between the most and the second most probable classes: arg min f D ( x ) also denotes the most probable class and f D second ( x ) de- notes the second most probable class. Taking a further look at the minimum confidence and minimum margins, the corresponding criteria for cost sensitive active learning can also be derived. (1) Maximum expected cost reduction: the minimum confidence can be rewritten as: Consider Err as the cost matrix C , we can get: This is also called the maximum expected cost reduction, which can minimize the expected cost. The object of the cost-sensitive learning process can be represented as: (2) Cost-weighted minimum margin: this is similar to the maximum expected cost reduction. By replacing the confidence term with the expected cost, we can get the cost sensitive version of the minimum margin: The detailed derivation of the above selective criterion for cost sensitive active learning can be found in [6] . The classification performance can be formulated by using a confusion matrix. For balanced classification problems, the accuracy and sensitivity are usually chosen as the key performance indexes, which are listed as follows: It should be noted that the sensitivity index is also called fault detection rate(FDR) in the more general fault diagnosis field. For imbalanced classification problem, G-mean is a popular evaluation index in general as it integrates the recalls of all categories [30] , which is defined as: G-mean tries to maximize the accuracy on each class while keeping these accuracy values balanced. Thus, a higher G-mean value indicates that the comprehensive performance of a classifier is better. For binary classification, G-mean is the square root of the product of sensitivity and specificity. For multi-class problems, it is a higher root of the product of sensitivity for each class. In this paper, we propose a cost sensitive active learning model using a bidirectional gated neural network framework for imbalanced fault diagnosis. The offline stage mainly consists of two steps: the training of sample-sensitive bidirectional gated neural unit and cost sensitive active learning for querying unlabeled samples. Next, we first briefly introduce the training of standard BGRU, and the training of sample-sensitive BGRU is given on this basis. Then, the process of cost sensitive active learning is discussed. Finally, the proposed cost sensitive active learning model using a bidirectional gated neural unit framework is formulated. In the training phase of BGRU, the orthogonal initialization method is adopted, and the Adam optimizer is used to train the network. In Algorithm 1 , the entire training procedure of BGRU is shown. The orthogonal initialization proposed in [31] is adopted as it can prevent vanishing and exploding gradients. When updating the network parameters, the Adam optimizer, which combines the advantages of two other popular methods, namely AdaGrad and RMSProp, is used [32] . When the performance achieved on the validation dataset cannot be improved any more, the learning rate is then delayed. In Algorithm 2 , we show the training procedure of samplesensitive BGRU. In this case, the sample weights need to be assigned. This can be done in the procedure below: first, we specify the class weights, and one of the recommended ways of doing so can be found in [33] ; second, the sample weights are assigned according to the class weights and the effect of time. Suppose a fault occurs at time t , assuming t = t 0 , then the weight of normal sample is: The weight of the fault sample is: where α 1 and α 2 denote the decay factors. This idea of assigning weights is mainly ascribed to the difficulty of distinguishing samples before and after a fault occurs. The closer to the time the fault occurred, the larger the weight of the sample should be. In this work, the weights of samples selected by the cost sensitive active learning are assigned the values of the class weights. A visual representation of the assigning process is shown in Fig. 3 . In order to explore the unlabelled data, the cost sensitive active learning is considered. The detailed procedure is shown in Algorithm 3 . The algorithm flow is similar to the ordinary active learning flow. First, we use the maximum expected cost reduction criterion to sample some instances from the unlabelled dataset. Then the labels of these selected will be assigned by human experts. The samples weights will be given according to their labels. In the next step, the selected samples will be added to the original training dataset to build an enhanced dataset. The enhanced dataset will then be used to train the model. The difference between our method and the classical active learning methods is in selecting the unlabelled data -in our method, class imbalance is taken into consideration. In this work, maximum expected cost reduction is adopted as the sampling strategy. Based on the algorithms introduced above, the complete imbalanced fault diagnosis framework proposed in this work is shown in Fig. 4 . The entire monitoring procedure is described as follows: • Offline Modeling 1. Obtaining the training dataset from normal operation and fault conditions. In this paper, the Tennessee Eastman(TE) Chemical benchmark process is firstly adopted to verify the effectiveness of the proposed method. TE is a simulation-based process using real data from a chemical engineering process [34] . The simulation model can be downloaded from the website: http://depts.washington.edu/ control/LARRY/TE/download.html . A detailed explanation of the TE process can be found in [35] . Normal data and fault data of the TE process are collected from the simulations on MATLAB 2016a. The sampling period is set to 36 seconds (i.e. 100 samples/h). For the training data, the simulator runs for 48 h in the normal state. 4800 normal training samples are then collected. In each simulation task of the 20 faults, the simulator also runs 48 h (i.e. 4800 training fault samples). It is noteworthy that the simulations of fault 6 shut down after 7 h in the fault state, hence each simulation for fault 6 only includes 7 h of fault data. For the testing data of each fault, the simulator runs for 8 h in the normal state at the beginning (i.e. 800 test normal samples). Then the corresponding fault disturbance is introduced and the simulator continues to run for 40 h. In this way, 40 h of fault data (i.e. 40 0 0 test fault samples) are collected in each simulation. All samples will be used in a balanced scenario while some samples will be randomly selected according to the imbalance ratio and the proportion of unlabeled data for imbalanced fault diagnosis. The experiment mainly consists of two parts. In the first part of this case study, BGRU is applied to diagnose the faults to show the necessity of considering the time series. In the second part of this case study, the proposed CSALBGRU method is applied to the diagnosis of imbalanced faults. The implementation is based the Python libraries Scikit-Learn [36] , Imbalanced-Learn [37] , Keras [38] and R Semi-Supervised Learning package [39] . The network structure we use is shown in Fig. 5 . The first BGRU layer consists of 100 neurons and the second BGRU layer consists of 200 neurons. The network structure is determined after several structures have been tried. It can be seen that such a simple network structure can reach promising results. Adam is used for optimization with 0.001 initial learning rate and a 20-epoch training phase. The reported results are obtained after the experimental procedure is repeated for 10 times. In the first part of this case study, BGRU is applied to diagnosing the faults to show the necessity of considering time series. It should be noted that this sub-experiment is completed under the circumstance in which class balance exists. We compare BGRU with the other six methods to evaluate its effectiveness. Four widely used conventional methods for fault diagnosis, namely PCA, KPCA, one-class SVM and two-class SVM, are adopted to verify the superiority of BGRU. Meanwhile, the deep convolutional neural network(DCNN), which is a typical deep learning technique, is also chosen for the comparison. BLSTM, which shares the same architecture and training parameters with BGRU, is also used for comparison. BLSTM can further demonstrate the generalization performance of BGRU is better than BLSTM. For multi-class fault diagnosis, we compared the BGRU with SVM and DCNN. One versus one strategy is proved to be a more suitable way for practical use than the other methods [40] . It is then used to extend the two-class SVM to multi-class SVM. In the second part of this case study, the proposed CSALBGRU method is applied to the diagnosis of imbalanced faults. In this sub-experiment, we will first set the imbalance ratio and the proportion of unlabelled data. And the training data selection procedure is as follows: (1) fault samples are randomly selected according to the imbalance ratio, i.e, the number of normal samples (4800) times imbalance ratio; (2) the corresponding proportion of normal samples and fault samples is set to be unlabeled. For the test data selection, the number of fault samples is the number of normal samples(800) times imbalance ratio and these fault samples will be randomly selected. We compare our proposed framework with four imbalanced learning methods and two semi-supervised learning methods. For imbalanced learning methods, as few researchers notice the class imbalance for fault diagnosis, two classical approaches, namely ADASYN [41] -SVM and RUSBoost [42] , and two deep learning based methods, namely deep laplacian auto-encoder(DlapAE) and generative neural network(GAN) are tested. For the semi-supervised methods, as some researchers have applied the semi-fisher discriminant analysis (Semi-FDA) for fault diagnosis, it is selected for comparison as well. Another popular semi-supervised method, the transductive support vector machine(TSVM) is also employed in the simulation for comparison. Table 1 shows that BGRU produces the best FDRs among these methods. The average FDR of BGRU is 96.6%, meaning an increase by 34.2%, 43.7%, 18.2%, 40.6%, 2.7%, 10.4%, 9.5% and 1.2%, compared with the SPE statistic of PCA, T 2 statistic of PCA, the SPE statistic of KPCA, T 2 statistic of KPCA, one-class SVM, two-class SVM, DCNN and BLSTM, respectively. It is also observed that the average FDR of BGRU is slightly higher than DCNN and BLSTM, so we use BGRU as the base classifier in the following discussion. Among the traditional methods, the performance of one-class SVM is the best and closest to that of BGRU. However, it is worth mentioning that the fault detection rates of fault 3, fault 9 and fault 15 for one-class SVM are not reported and it may indicate that the one-class SVM does not achieve good diagnostic results in these faults. The fault detection rates of fault 3, fault 9, fault 15 for BGRU are 98.4%, 93.9% and 66.2%, respectively. We compare BGRU with multi-class SVM and DCNN for further analysis of their performances in multi-class faults diagnosis. In this case, fault 1, fault 2, fault 3, fault 8, fault 10, fault 11, fault 12, fault 13, fault 14, and fault 20 are selected. The twodimensional codes for each class of BGRU is shown in Fig. 6 . It is observed that BGRU produces an excellent visualization effect of the data. It is not hard to speculate that the softmax layer can get a promising classification result. The confusion matrices obtained Fig. 6. BGRU-learned features. Specifically, we using t-sne reduce the dimension of the feature (input of the fully connected layer) to 2, and then plot them by class. by SVM, DCNN and BGRU are illustrated in Figs. 7 and8 . For the confusion matrix, the rows show the predicted fault mode IDs, the columns show the actual fault mode IDs. The values on the diagonal of the matrix represent where the true fault and the predicted one match. The values on the non-matrix diagonal represent the percentage of instances where the algorithms have made mistakes. It is noticed that the accuracy value for each class of BGRU is all above 90% while the accuracy values of some classes of SVM and DCNN are much less than 90% (e.g. fault 12 of SVM, 24.12% and fault 12 of DCNN, 58.8%). The overall accuracy of BGRU is 95.5% while the overall accuracy values of SVM and DCNN are 79% and 87.7% -this shows a remarkable increase by 21% and 8.9%, respectively. At the beginning of this sub-experiment, we set the imbalance ratio to be 0.5 and the proportion of unlabelled data is 0.6. We first prove the effectiveness of cost sensitive active learning using fault  4 and fault 15 as the test cases. We first compare the influence of the number of selected samples during each step of iteration on the experimental results. We chose 5,10 and 15 samples for one step of iteration. The detailed results are shown in Fig. 9 . It can be found that the G-mean of fault 4 increases from 50% to 70% when 10 samples are selected in each query iteration while the G-mean increases from 50% to 68% when either 5 or 15 samples are selected in each iteration. This means the labeling cost can be reduced and the final performance can increase if an appropriate number is selected. Choosing to label more samples each time may grow faster in the first few iterations, but it would converge too quickly. It is also demonstrated that cost sensitive active learning can help improve the fault diagnosis performance greatly by using only a small amount of samples, i.e, a total of 50 samples in the experiment, accounting for 1% of the unlabelled data. That similar results can be found as well when fault 15 is tested. Then the cost sensitive criterion is tested with the other three common criteria: entropy sampling, margin sampling and uncertainty sampling. We select 10 samples in each iteration. The comparison results are shown in Fig. 10 . It can be seen that the proposed method obtains the highest G-mean value among all methods in each iteration. Take fault 4 as an example, the final Gmean of our method is about 70% while the other three methods are about 67%. In every query iteration, the G-mean of our method is higher than the other three methods. It is indicated that the proposed method precisely selects the most informative samples. The similar results can be found in the diagnosis of fault 15 as well. The comparison results of CSALBGRU and ADASYN-SVM, RUSboost, semi-FDA, TSVM, DlapAE and GAN are shown in Table 2 . It is observed that the CSALBGRU shows remarkably improved performance in all 20 faults. It is noticed that the FDRS of ADASYN-SVM and RUSBoost are lower than CSALBGRU, and the reasons are twofold. First, the algorithms are limited to imbalanced learning. Second, the algorithms do not take advantage of unlabeled data. The FDRs of semi-FDA and TSVM are also lower than CSALBGRU and we also note that the FDRs for some faults are 0 as these methods prefer the majority classes. For the deep learning-based methods, DlapAE and GAN, their FDRs are higher than the traditional methods and lower than our proposed method in most faults. For the DlapAE based methods, they can learn a smooth manifold structure of data, but cannot solve the problem of imbalanced classes. And for GAN-based methods, the generated samples are of poor quality since the training process of GAN is not stable. These reasons can to some extent explain the reasons why the results obtained by these methods are not as good as those obtained by CSALBGRU. We also compared the proposed techniques with ADASYN-SVM, RUSBoost, semi-FDA, TSVM, DlapAE and GAN under different imbalanced ratios. The proportion of fault 10 data to normal data is set from 0.1 to 0.6 with a step size of 0.1. The results are illustrated in Fig. 11 , showing that the proposed CSALBGRU method maintains a significant advantage all the time compared with the traditional methods. It is indicated that the idea of weighting each sample and using cost sensitive active learning to explore unlabeled data is prone to improve the discrimination of minority faults. Compared with other methods, the performance of the proposed method is almost unaffected by the proportion of fault data. When the proportion of fault data is small, the semi-supervised methods fail to distinguish the minority fault completely while the imbalanced learning methods can deliver considerably good results. This further proves that the proposed method achieves the best performance. It should be noticed that the GAN based method obtains   The proposed method is also applied to multi-class fault diagnosis. One versus one strategy is used to extend ADASYN-SVM and RUSboost for multi-class classification. The incremental G-mean of CSALBGRU is shown in Fig. 12 . The results show that it is not always better to choose more samples for each iteration. A relatively large number would yield better results. In this case, selecting 100 samples in every iteration has better performance than selecting 50 or 150 samples in every iteration. The proposed method has the best performance compared to other three methods. This provides  It is worth mentioning that the proposed method shows better performance again. Hence the stability and robustness of the proposed method are further demonstrated. In this section, a practical case study is used to further illustrate the effectiveness of the proposed method. The data are obtained from a plasma etching process in a real-world semiconductor packaging production line. A detailed description of the plasma etching process can be found at [48] . During the plasma etching process, a fault called the micro-arcing fault may occur. Its occurrence is mostly unnoticed and can seriously affect product quality. Therefore, it is necessary to detect faults for this process in a timely manner. The process involves 12 key variables, as listed in Table 3 . The dataset used in this case study contains 22,605 labeled samples, of which 19,762 are normal samples and 2843 are fault samples. In addition, there are also 2516 unlabeled samples, meaning that this is a typical imbalanced fault diagnosis problem. After  normalization, we also randomly choose 80% of the labeled samples and all unlabelled samples as the training samples for imbalanced fault diagnosis. The last 20% of the labeled samples are used for testing. We compare our proposed method with six methods, namely ADASYN-SVM, RUSBoost, Semi-FDA, TSVM, DlapAE and GAN. The network structure is the same as described in Fig. 5. The results of comparison are shown in Table 4 . It is observed that the proposed CSALBGRU achieves the best performance among these methods in both the training dataset and test dataset. It is worth mentioning the fault diagnosis performance of the GAN based method on the test set is not as good as that on the training set. This indicates that the GAN based methods may suffer from the overfitting problem. In this paper, a cost sensitive active learning using bidirectional gated recurrent neural network is proposed for imbalanced fault diagnosis. The dynamic behaviour of the process is first tackled using a bidirectional gated recurrent neural network. Then sample sensitive learning is developed to reduce the influence of class imbalance. Finally, cost sensitive active learning is utilized to explore the unlabeled data so that waste of data can be eliminated. The effectiveness of the proposed technique is evaluated in a set of computational experiments using the Tennessee Eastman (TE) dataset and a practical dataset. It is demonstrated in the experiments that the proposed method possesses a significant improvement compared to the existing methods. The authors' future work will be focused on two main areas. First, deep learning techniques for semi-supervised learning will be studied for using the unlabeled data more effectively. Second, some researchers have begun to pay attention to the cases where fault samples are missing [49,50] . For example, Hu et al. proposed a novel incremental imbalance modified deep neural network (incremental-IMDNN) to address the continuous arrival of new fault modes [50] . Xu et al. proposed the renewable fusion fault diagnosis network (RFFDN) to handle the extreme case. These two methods have the common idea of sharing weights, meaning that transfer learning based methods may achieve promising performance in the case. This will be a particular area that the authors will explore as well. The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Hongwei Wang: Writing -review & editing, Funding acquisition. Heming Zhang: Funding acquisition, Supervision.",
  "filePath": "C:\\Users\\Alex Isaev\\Documents\\truth source\\citation-verifier\\src\\document-database\\documents\\cost_sensitive_active_learning_using_bidirectional_gated_recurrent_neural_networ.json",
  "sourcePdf": "C:\\Users\\Alex Isaev\\Documents\\truth source\\citation-verifier\\src\\document-database\\pdf-documents\\22.pdf",
  "doi": "10.1016/j.neucom.2020.04.075",
  "year": "2020",
  "journal": ""
}