{
  "id": "polynomial_fitting_algorithm_based_on_neural_network",
  "title": "Polynomial Fitting Algorithm Based on Neural Network",
  "authors": [
    "YuerongTong",
    "LinaYu",
    "ShengLi",
    "JingyiLiu",
    "HongQin",
    "WeijunLi"
  ],
  "content": "Index Terms: Polynomial fitting, neural network, Taylor series, Implicit polynomial representations. -------------------------------------------------------------------------------------------------------------------------------------1 With the development of Internet technology, various fields of scientific research are flooded with large amounts of data. For example, in the health sector will have a lot of medical diagnostic data, the financial sector will have a lot of capital flows data, the computer graphics field will generate a lot of graphical image data [1,2]. Therefore, people are increasingly concerned about how these growing massive amounts of data effectively organize and manage, it has also become the target research experts and scholars at home and abroad to focus on. The fitting of polynomial functions has always been the main research hotspot in mathematical modeling. Polynomial fitting generally includes explicit polynomial fitting and implicit polynomial fitting. As a method of function approximation, polynomial fitting has been applied in many fields [3,4]. By choosing different fitting methods to fit the discrete points in the data set, the errors produced by the fitting are also different. Therefore, choosing a suitable fitting method has become the foundation of polynomial fitting research. In order to meet the needs of various fields, more and more experts and scholars have launched the research on polynomial fitting. Explicit polynomial fitting often uses Taylor series [5,6]. Taylor series are often used in the field of mathematics, especially in the research of approximate calculations. In mathematics, in order to realize the expression of a specific function, a series of terms is obtained by deriving the function at a certain point, and the Taylor series is obtained by adding these terms. In recent years, implicit polynomial curves have been widely used in computer graphics [7,8], computer vision [9,10], time series [11,12] and so on. The implicit polynomial curve can realize the description of irregular objects with few parameters, and it has a clear analytical formula and is easy to use [13]. For implicit polynomial fitting, iterative fitting algorithms are often used in the early days. But the iterative fitting algorithm has a common disadvantage that the polynomials generated are not stable. When fitting complex object contours, the iterative fitting algorithm will cause a large time cost due to the large amount of calculation. Therefore, the early iterative fitting algorithm is only suitable for fitting the contours of simple objects. In recent years, related research scholars have also carried out further research on this [14,15]. At present, some mainstream fitting algorithms such as the general linear least squares fitting algorithm [16] and the 3L curve fitting algorithm [17] based on the improved linear least squares method. Compared with the earlier iterative fitting algorithms, these algorithms have better performance in fitting the contours of complex objects, and the time overhead in the calculation process is smaller. For traditional polynomial fitting methods, the theoretical analysis is very rigorous, but the theoretical derivation is more complicated. Some algorithms derived from this have higher requirements for raw data and poor adaptability. Neural networks have been widely used in many disciplines [18,19]. The application of neural networks to polynomial fitting has advantages different from traditional methods. The main contributions of this article are as follows: 1) This paper constructs the neural network topology structure based on the polynomial fitting model, uses Taylor series as the activation function of the network, and uses the original experimental data samples to train the model parameters to obtain the optimal fitting parameters. 2) This paper uses the gradient backpropagation algorithm, which can effectively solve the coefficients of the polynomial function without complicated formula derivation. To a certain extent, the complexity of polynomial fitting is reduced. 3) This paper has carried out experimental verification on explicit polynomial fitting and implicit polynomial fitting, and compared the current popular polynomial fitting algorithms to prove the effectiveness of the algorithm. In mathematics, the algebraic expression formed by adding several monomials is called a polynomial, which can be recorded as = 0 0 + 1 1 + 2 2 + ⋯ + , where 0 , 1 , ⋯, is the coefficient of the polynomial. Each monomial is the term of the polynomial, and the highest degree is called the order of the polynomial. Polynomial fitting uses a polynomial expansion to fit discrete data points. It does not require the polynomial curve to pass through all data points. The purpose of polynomial fitting is to find a set of 0 , 1 , ⋯, so that the polynomial curve matches the actual sample data as much as possible. Explicit polynomial fitting often uses Taylor series. Take a univariate function as an example. Taylor's theorem: Let be a positive integer. Let be a positive integer. If a function defined on an interval containing is differentiable + 1 times at the point , then for any on this interval, there is: Where ( -) is the remainder. Taylor's theorem describes a differentiable function. At a certain point, the derivative of each order of the function is obtained, and the derivative value is used as the coefficient of the polynomial, and the polynomial is used to approximate the function value of the function in the neighborhood of this point. From this we know that Taylor's formula can use polynomials to approximate the situation around a certain point. Therefore, Taylor series are often used for explicit polynomial fitting. Implicit polynomial curve is a special kind of algebraic curve, which has precise expressive ability, has clear analytical formula and is therefore very easy to use, and can describe the outline of data point collection well. Therefore, the implicit polynomial fitting algorithm has become the key research direction of related researchers, and the research on it has a realistic background basis. A binary implicit polynomial function is shown in formula (2). The research results show that the zero set composed of binary implicit polynomials can represent arbitrarily complex curves or surfaces in two-dimensional space. An implicit polynomial function is usually written as: Formula ( 3) is expressed in vector form, ) is a vector of items. Use implicit polynomial fitting. When the contour of the fitted object is more complicated, if you choose a low-order implicit polynomial, the curve obtained by fitting cannot well reflect the edge characteristics of the contour of the object, and the fitting effect is not good; The contour of the object is relatively simple. If you choose a higher-order implicit polynomial, it will not only greatly increase the time required in the fitting process, but also make the obtained implicit polynomial more complicated and difficult to handle, and it will be more difficult in the future application process. The correct choice of the order of the implicit polynomial is extremely important in the process of fitting the contour of the object. There have been many research scholars on how to determine the fitting order of implicit polynomials. The research results point out that when the order of the implicit polynomial is even, the zero set can be bounded or unbounded; when the implicit polynomial is an even number, the zero set can be bounded or unbounded; When the order of is odd, the zero set is always unbounded. Therefore, even order is mostly used in implicit polynomial fitting. Choosing the right number of hidden layer nodes is very important for neural networks It not only has a great impact on the performance of the established neural network model, but also is the direct cause of overfitting during training. The number of hidden layer nodes has a great influence on the iteration speed of the neural network. The fewer hidden layer nodes, the faster the iteration speed of the neural network. However, if the number of hidden layer nodes is too small, the neural network cannot achieve the effect of learning and approximation, and the network performance is very poor; if the number of hidden layer nodes is too large, the neural network may appear overfitting, etc. Undesirable phenomena will also make the hardware implementation and software calculation of the neural network more complicated. Under normal circumstances, it is necessary for the experimenter to obtain the most suitable number of hidden layer nodes of the neural network based on experience and a large amount of experimental exploration. In order to avoid the phenomenon of overfitting in the training process of the neural network as much as possible, on the basis of correctly satisfying the inputoutput relationship, the number of hidden layer nodes is selected as few as possible. Because this paper uses neural network to fit polynomial functions, according to the structural characteristics of polynomials, we can accurately select the number of hidden layer nodes in the process of specific experiments, instead of selecting the number of hidden layer nodes through continuous exploration. In other words, if the number of inputs is known, the number of hidden layer nodes can be accurately represented by the polynomial order . That is, when the number of inputs is 1, the number of hidden layer nodes required is + 1 , and when the number of inputs is greater than 1, the number of hidden layer nodes in the neural network is ( +3) 2. Kolmogorov's theorem is the mathematical foundation of neural networks [20]. According to Kolmogorov's theorem, it can be known that given any continuous function, the function can be accurately implemented with a three-layer feedforward network. Under the condition that the number of hidden layer nodes can be selected arbitrarily, the three-layer neural network can approximate any continuous function with arbitrary precision under the action of the activation function. Series refers to the form and sum of a finite or infinite sequence. According to the definition of series, a neural network can be regarded as the form and sum of a series of neuron activation functions. The process of neural network training is to gradually correct the series coefficients of these activation functions. Taylor's theorem states that for a function that is differentiable and smooth enough, the derivative values of a certain point can be used as the coefficients of the polynomial function. Then use this coefficient to approximate the value of the function in the neighborhood of this point. In the experiment, the zero point is selected as the expansion point of the Taylor series, and the weight of the neuron corresponds to the Taylor coefficient. In this paper, a three-layer feedforward neural network is used to create a Taylor series neural network model. Take the Taylor series of the unary function as an example. In order to describe the finite Taylor series, considering the sum of the first + 1 terms, we can get formula (5). According to formula (5), we can get the neural network model as shown in Figure 1. The bias of each neuron in the hidden layer of the neural network is 0, and the weight between the neurons in the hidden layer and the neurons in the input layer is set to a constant: = 1( = 1,2, ⋯, ). For each neuron in the hidden layer of the neural network, its output is shown in formula (6). The output of the neural network is shown in formula (7). We can get the first + 1 coefficient of Taylor expansion, and get the weight ' and bias by training the neural network. When using a polynomial to fit the contour of an object, if the distance between the point on the given original data set and the origin of the coordinate is too large, the polynomial coefficient will increase exponentially. Not only greatly increases the calculation time of the algorithm, but also inconvenient for future applications. Therefore, we must first normalize the data. The data points are concentrated near the origin of the coordinates, which is conducive to the subsequent fitting. In the process of neural network training, if the amount of training data is small, the neural network may learn some irrelevant features due to its strong fitting ability, but the characteristics of the data itself cannot be learned. Therefore, it is necessary to perform enhancement operations on the training data. The data enhancement operation can expand the original data set by introducing some transformations without substantially increasing the data. In this way, limited data can generate value equivalent to more data. In order to improve the fitting ability of the neural network training model, the data is scaled equally using the idea similar to the 3L algorithm. We call this operation 3L scaling. First multiply the normalized original data by a scaling factor ( > 0) , and use the scaling factor as the value of the implicit polynomial function. The specific operation is shown in formula (9). ( The 3L scaling is different from the D-Euclidean distance transformation in the 3L algorithm. But in the experiment, this method can achieve similar experimental results, which also simplifies the tedious and complicated calculation process in the 3L algorithm. Let the scaling factor = 0.05, the zoomed effect diagram is shown in the figure 3. For explicit polynomial fitting, a non-linear function is often selected as the objective function. For the description and fitting of object contours, implicit polynomial curves have natural advantages compared with explicit polynomial curves. Implicit polynomial fitting mostly uses the target in Figure 4 as the fitting object, and there is no public data set for the twodimensional target curve. After detecting the edge of the original image by using edge detection technology, sampling is performed to obtain the sample points as shown in Figure 5. For explicit polynomial fitting, one-variable polynomial is the simplest and most basic form of the approximated function. Take function = sin ( ) as an example to study the effect of the algorithm used in this paper on the fitting effect under the same number of iterations and different orders of polynomial selection. The learning rate in neural network training is set to 0.03, and the number of iterations of the neural network is 5000 times. The fitting results of the polynomial order from the second to the fifth order and the corresponding loss function are shown in Figure 6. Then we perform a binary explicit polynomial fitting on the saddle surface, the conical surface and the Gaussian surface. The formula for the saddle surface is shown in formula (10). The modeling effect of 5000 data points of the saddle surface under random sampling conditions is shown in Figure 8. In the experiment, the saddle surface was fitted under the second and third order respectively. The learning rate is 0.03, and the number of iterations is 1000. The specific parameter settings in the experiment are as follows: = 3, = 3, = The formula for conical surface is shown in formula (11). Model 5000 data points of the conical surface under random sampling conditions. In the experiment, the conical surface is fitted under the third and fourth order respectively. The learning rate is 0.03, and the number of iterations is 1000. The specific parameter settings in the experiment are as follows: = 3 , = 3 , = 1 . Take the data points of the conical surface in the positive direction of the vertical axis of the coordinate axis to conduct the experiment. Figure 9  The formula of Gaussian surface is shown in formula (12). Model 5000 data points of the Gaussian surface under random sampling conditions. The specific parameters of the Gaussian surface in the experiment are set as follows: = 0.5,0.1 , = 0.8,0.08 , = 0.0 1.5 0.0 0.0 , 1 = 0.008, 2 = 10 . In the experiment, the Gaussian surface is fitted in the second to sixth order. The learning rate is 0.05, and the number of iterations is 6000, 6000, 6000, 10000, and 10000 respectively. The fitting results and loss function under different orders are shown in Figure 10. For implicit polynomial fitting, in order to study the influence of the polynomial order on the fitting effect and find the most suitable order for fitting the target curve, we take the butterfly curve as an example to conduct experiments. The research results point out that when the order of the implicit polynomial is even, the zero set can be bounded or unbounded; when the order is odd, the zero set is always unbounded. In an ideal situation, as the order of the polynomial increases, the polynomial fitting ability will be stronger. Therefore, in the specific experiment process, even-order polynomials are used to fit the target curve. The butterfly curve is fitted under the second, fourth, and sixth order. The fitting results and loss function are shown in Figure 11. The red curve is the original data points, and the blue curve is the fitting result. By comparing the fitting results of the butterfly curve under different orders, we can find that as the order of the polynomial increases, the fitting effect of the polynomial is better. Although the sixth-order polynomial has better fitting ability than the fourth-order polynomial, it can be seen from the fitting results in Figure 11 that the higher-order terms of the polynomial will amplify the unsmooth interference points and affect the final fitting result. Next, we explore the fitting effect of the polynomial fitting algorithm based on neural network, 3L curve fitting algorithm and general linear least squares fitting algorithm under the fourth order. The results are shown in the figure 12. (a) Through experiments, it can be found that the resulting curves obtained by the general linear least squares fitting algorithm under the fourth-order polynomial cannot be very close to the original data. Therefore, the general linear least squares fitting algorithm is not suitable for complex curve fitting. For most curves, the difference between the fitting results of the polynomial fitting algorithm based on neural network and the 3L curve fitting algorithm is little. But for the fitting of shoes and guitar curves, the polynomial fitting algorithm based on neural network can fit the closed The curve, and the fitting curve obtained by using the 3L curve fitting algorithm is open and incomplete. The following explores the effect of the polynomial fitting algorithm based on neural network and the 3L fitting algorithm in the presence of noise. Taking the butterfly curve as an example, add 20 noise points, 40 noise points, and random disturbances to the original butterfly data points. The fitting results are shown in Figure 13, where the first line uses the results obtained from the neural network-based polynomial fitting algorithm, and the second line uses the results obtained from the 3L curve fitting algorithm. In order to fit the polynomial, the neural network-based polynomial fitting algorithm used in this paper constructs a three-layer feedforward artificial neural network, and uses Taylor series as the activation function of the network. For explicit polynomial fitting, this paper uses a variety of nonlinear functions as the objective function, and compares the fitting effects under different orders of polynomials. For the fitting of implicit polynomial curve, it can be found through experiments that the general linear least squares fitting algorithm has a particularly poor fitting effect. Under the condition of the original data, the fitting results of the algorithm used in this paper and the 3L curve fitting algorithm are very similar from a visual point of view, but by adding random noise, it can be found that the fitting result of the 3L curve fitting algorithm is not stable enough. Moreover, in the specific experimental process, inappropriate selection of the zoom factor of the original data set and the new data set will cause the polynomial curve to oscillate, making the fitting effect unsatisfactory, especially in the fitting of high-order polynomial curves. Therefore, it can be proved that the neural network-based polynomial fitting algorithm used in this paper is suitable for implicit polynomial fitting, and has good robustness, and can efficiently achieve the fitting goal, while the computational complexity is low.",
  "filePath": "C:\\Users\\Alex Isaev\\Documents\\truth source\\citation-verifier\\src\\document-database\\documents\\polynomial_fitting_algorithm_based_on_neural_network.json",
  "sourcePdf": "C:\\Users\\Alex Isaev\\Documents\\truth source\\citation-verifier\\src\\document-database\\pdf-documents\\33.pdf",
  "doi": "43188DF8A024F039B87223E5DFE853BC",
  "year": "2021",
  "journal": ""
}