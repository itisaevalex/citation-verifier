{
  "id": "online_robust_principal_component_analysis_with_change_point_detection",
  "title": "Online Robust Principal Component Analysis With Change Point Detection",
  "authors": [
    "WeiXiao",
    "Senior Member, IEEEXiaolinHuang",
    "FanHe",
    "Member, IEEEJorgeSilva",
    "SabaEmrani",
    "ArinChaudhuri",
    ")WXiao",
    "was with SAS Institute\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t27513\n\t\t\t\t\t\t\t\t\tCary\n\t\t\t\t\t\t\t\t\tNC\n\t\t\t\t\t\t\t\t\tUSA",
    "Institute of Image Processing and Pattern Recognition\n\t\t\t\t\t\t\t\tAmazon, Inc\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t98109\n\t\t\t\t\t\t\t\t\tSeattle\n\t\t\t\t\t\t\t\t\tWA\n\t\t\t\t\t\t\t\t\tUSA",
    "Institute of Medical Robotics\n\t\t\t\t\t\t\t\tMOE Key Laboratory of Sys\n\t\t\t\t\t\t\t\tShanghai Jiao Tong Univer- sity\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t200240\n\t\t\t\t\t\t\t\t\tShanghai\n\t\t\t\t\t\t\t\t\tChina",
    "tem Control and Information Processing\n\t\t\t\t\t\t\t\tSAS Institute Inc\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t200240 27513\n\t\t\t\t\t\t\t\t\tShanghai Cary\n\t\t\t\t\t\t\t\t\tNC\n\t\t\t\t\t\t\t\t\tChina USA",
    "S. Emrani was with SAS Institute Inc\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t27513\n\t\t\t\t\t\t\t\t\tCary\n\t\t\t\t\t\t\t\t\tNC\n\t\t\t\t\t\t\t\t\tUSA",
    "Apple Inc\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t95014\n\t\t\t\t\t\t\t\t\tCupertino\n\t\t\t\t\t\t\t\t\tCA\n\t\t\t\t\t\t\t\t\tUSA"
  ],
  "content": "outliers. Even with one grossly corrupted entry, the low-rank subspace that is estimated from classic PCA can be arbitrarily far from the true subspace. This shortcoming puts the application of the PCA technique into jeopardy for practical usage because outliers are often observed in the modern world's massive data. For example, data collected through sensors, cameras, and websites are often heavily noisy and contain error entries or outliers. Various versions of robust PCA have been developed in the past few decades, see, e.g., [6], [10]- [13]. Among them, the Robust PCA based on Principal Component Pursuit (RPCA-PCP) [6], [14] is the most promising one, as it shares both good practical performance and solid theoretical analysis. RPCA-PCP decomposes the observed matrix into a low-rank matrix and a sparse matrix by principal component pursuit. Under a mild condition, both the low-rank matrix and the sparse matrix can be recovered exactly. A comprehensive review of the application of RPCA-PCP on surveillance video can be found in [8], [15]. RPCA-PCP has also been extended to handle high dimensional data with randomly sampled rows/columns [16], [17]. Some works finds the robust subspace by optimizing a slightly different L1-norm maximization criterion [18], [19], and have been extended to compressed-sensed surveillance videos [20]. Other related interesting works could be found in [21]- [25]. Most robust PCA methods, including RPCA-PCP, are implemented in a batch manner, which requires all data to be available before processing. Thus, for big data, these methods become infeasible on both memory storage and computational time. An online version of robust PCA method is highly necessary to process incremental data, where the tracked subspace can be updated whenever a new sample arrives. Only with online version, robust PCA is applicable to video analysis, change point detection, abnormal events detection, and network monitoring. These are the main existing online robust PCA methods: Grassmannian Robust Adaptive Subspace Tracking Algorithm (GRASTA, [26]), Recursive Projected Compress Sensing (Re-ProCS, [27]- [29]), and Online Robust PCA via Stochastic Optimization (RPCA-STOC, [30]). GRASTA applies incremental gradient descent method on Grassmannian manifold for robust PCA. Its foundation is Grassmannian Rank-One Update Subspace Estimation (GROUSE, [31]), a widely used online subspace tracking algorithm. GRASTA uses the 1 cost function to replace the 2 cost function in GROUSE. ReProCS is designed mainly for foreground and background separation for video sequence, of which the key ideas are on perpendicular projection and sparse recovery. RPCA-STOC is based on stochastic optimization on a reformulation of RPCA-PCP. Specifically, RPCA-STOC splits the nuclear norm of the low-rank matrix as the sum of Frobenius norm, and iteratively updates the low-rank subspace to find the sparse vector. GRASTA and ReProCS are only able to handle slowly changing subspaces; while RPCA-STOC works only with a stable subspace. However, in practice, the low-rank subspace might change suddenly, and the corresponding change points are usually of great interest. For example, in the application of background subtraction from video, each scene change corresponds to a change point in the underlying subspace. Another application is in failure detection of mechanical systems based on sensor readings from the equipment. When a failure happens, the underlying subspace is changed. Accurately identifying the change point is useful for failure diagnosis. Other applications include human activity recognition, intrusion detection in computer networks, and so on. To handle both slowly and abruptly changing subspaces, in this paper we propose an efficient online and robust method, called Online Moving Window Robust Principal Component Analysis (OMWRPCA). It embeds hypothesis testing into an online robust PCA framework and then identifies the change points of the underlying subspace, by which the low-rank subspace and sparse error are estimated at the same time. To the best of the authors' knowledge, OMWRPCA is the first algorithm that is able to simultaneously detect change points and compute RPCA in an online fashion. It is also the first online RPCA algorithm that can handle both slowly changing and abruptly changing subspaces. Our method is also closely linked with Low-Rank Representation [32], [33], which recovers the authentic data that lie on a union of multiple subspaces with outliers. Recently, a new Robust Subspace Tracking (RST) method, namely (NORST, [34]) has been invented, which is equipped with change point detection capability. However, unlike our method, NORST can only handle piecewise constant subspaces and it requires the subspaces to be of a fixed rank. Moreover, it is not able to locate the exact position of a change point as it searches for a change point every α observations (where α is a parameter set by the user and usually takes a value above 50). The remainder of this paper is organized as follows. Section II describes the problem and reviews the related methods. The OMWRPCA algorithm is developed in Section III. In Section IV, the proposed methods are evaluated on extensive numerical experiments, including an application on real-world video surveillance data. Section V ends the paper with conclusions and discussions. In this paper, bold letters stand for vectors and matrices, a 1 and a 2 for the 1 -norm and 2 -norm of a vector a, respectively, A F for the Frobenius norm of matrix an arbitrary real matrix A, A * for the nuclear norm (sum of all singular values), and A 1 for the 1 -norm. Let T denote the number of observed samples, and t the index of the sample instance. We assume that the inputs are Robust Principal Component Analysis based on Principal Component Pursuit (RPCA-PCP, [6], [14]) which decomposes the observed matrix M into a low-rank matrix L and a sparse matrix S by principal component pursuit: Theoretically, L and S can be recovered exactly if 1) the rank of L and the support size of S are small enough; 2) L is sufficiently \"dense\"; 3) any element in the sparse matrix S is nonzero with probability ρ and 0 with probability 1ρ independent over time [6], [26]. RPCA-PCP is able to estimate a low-rank matrix to approximate the observed data when the inputs are corrupted with gross but sparse errors. Various algorithms have been developed to solve RPCA-PCP, including Accelerated Proximal Gradient (APG, [35]) and Augmented Lagrangian Multiplier (ALM, [36]). According to experiments reported by [6], ALM achieves higher accuracy than APG, in a fewer iterations. Accordingly, ALM is utilized in this paper and is outlined in Algorithm 1. In Algorithm 1, S τ [x] = sgn(x) max(|x|τ, 0) denotes a shrinkage operator and S τ is extended to matrices by applying it to each element. Besides, D τ (X) denotes the singular value thresholding operator given by D τ (X) = U S τ (Σ)V * , where X = U ΣV * is any singular value decomposition (SVD). As a batch algorithm, RPCA-PCP iteratively solves SVD with all data. Thus, it requires a huge amount of memory and does not scale well with big data. In practice, the underlying subspace can change over time, which makes RPCA-PCP infeasible as the rank of matrix L (which includes all observed samples) will increase over time. One solution is to apply a sliding window to the original data, resulting in Moving Window Robust Principal Component Analysis (MWRPCA), which iteratively calculates batch RPCA-PCP using only the latest n win data column vectors, where n win is a user specified window size. MWRPCA generally performs well for subspace tracking with the capability of tracking on both slowly changing subspaces and abruptly changed subspaces. However, MWRPCA often becomes too slow to deal with real-world big data problems. For example, to track the subspace on a 200 × 10000 matrix, if the window size is set to be n win = 1000, we need to run RPCA-PCP 9000 times on 200 × 1000 matrices, which is computationally very expensive. An online algorithm is proposed in [30] for robust PCA-PCP by solving the following problem where λ 1 , λ 2 are tuning parameters. The main difficulty in developing an online algorithm for the above problem is that the nuclear norm couples all the samples tightly. Following the discussions in [37], [30] considers the problem based on the equivalence for the nuclear norm of a matrix L with a upper bounded rank r: where U can be seen as the basis for low-rank subspace and V represents the coefficients of observations with respect to the basis. Based on this observation, a RPCA-STOC algorithm is developed in [30], which minimizes the empirical version of ( 4) and processes one sample per time instance. Given a finite set of samples where the loss for each sample is defined as follows: With fixed U = U t-1 , v t and s t can be obtained by solving the following optimization problem: Then with known {v i , s i } t i=1 , the basis U t is updated by minimizing the following function: of which the minimum can be explicitly obtained as follows: . In practice, U t can be quickly updated by block-coordinate descent with warn restarts. The details of the RPCA-STOC algorithm can be found in [30]. RPCA-STOC strongly assumes the existence of a unique stable subspace, which is generally too strict and excludes many real-world applications such as failure detection in mechanical systems, intrusion detection in computer networks, fraud detection in financial transaction, and background subtraction for surveillance video. The essential problem is that RPCA-STOC updates the basis of subspace by minimizing an empirical loss involving all previously observed samples with equal weights, from which it follows that the obtained subspace is actually an \"average\" subspace of observations. This is clearly not suitable if the underlying subspace is changing over time. To deal with this difficulty, we propose a new method which combines the idea of moving window RPCA and RPCA-STOC to track changing subspaces. At time t, we update U t by minimizing an empirical loss based only on the most recent n win samples. Here n win is a user specified window size and the new empirical loss is defined as The proposed method is called Online Moving Window RPCA (OMWRPCA). Compared with RPCA-STOC, the major advantage of OMWRPCA is that OMWRPCA can quickly update the subspace when the underlying subspace is changing. Moreover, it has two other differences: 1) In RPCA-STOC, the dimension of the subspace, namely r, should be known. In OMWRPCA, we estimate r by computing batch RPCA-PCP with burn-in samples. 2) The number of burn-in samples n burnin is a user specified parameter in RPCA-STOC. While, we use the estimated U from the burn-in samples in OMWRPCA. The basic algorithm for OMWRPCA is summarized in Algorithm 2. One limitation of the basic OMWRPCA is that it can only handle slowly changing subspaces. When a subspace changes suddenly, the basic OMWRPCA algorithm will fail to update the subspace correctly. This is because when the new subspace is dramatically different from the original one, online updates of the subspace might not be possible or it may take some time to finish the updates. A special case is when the new subspace is in a higher dimension space than the original subspace, the basic OMWRPCA algorithm is unable to update the subspace correctly because the rank of the subspaces are held fixed during updates. This drawback is suffered by almost all the existing online RPCA algorithms. Therefore, change point detection is very crucial for subsequent identification, and an online RPCA algorithm that can correctly identify a change point is very desirable. In fact, users sometimes are more interested in pinpointing the change points than in estimating the underlying subspace. Thus, we designed a variant of our OMWRPCA algorithm to simultaneously detect change points and compute RPCA in an online fashion. The algorithm is robust to dramatic subspace changes. Since this goal is achieved by embedding hypothesis testing, we call the new algorithm OMWRPCA-CP. OMWRPCA-CP is based on a simple yet important observation: when the new observation can not be modeled well with the current subspace, a sparse vector ŝt with an abnormally large support size ĉt will be estimated from the OMWRPCA algorithm. So by monitoring the support size of the sparse vector calculated from OMWRPCA, we can identify the change point of the subspace, which is usually the starting point of level increases in the time series of ĉt .  In each section, we have a constant underlying subspace whose dimension is given by r. The elements of the true sparse vector have a probability of ρ of being nonzero and are independently generated. The dimension of the samples is 400. We choose a window size of 200 in OMWRPCA. Theoretically, when the underlying subspace is accurately estimated, the support sizes of the estimated sparse vectors ĉt should be around 4 and 40 for the ρ = 0.01 case and the ρ = 0.1 case, respectively. For the upper two cases in Fig. 1, ĉt blows up after the first change point and never returns to a value in the normal range. This is because after fitting RPCA-PCP on burn-in samples, the dimension of U t is fixed as 10. Thus, the estimated subspace U t cannot approximate well the true subspace in the later two sections of the timeline as the true subspaces have a dimension larger than the fixed dimension. On the other hand, for the lower two cases in Fig. 1, ĉt blows up immediately after each change point, and then drops back to the normal range after a while when the estimated subspace has converged to the true new subspace. We now give an intuitive explanation for the above phenomenon. At time t, the observation is m t = U t v t + s t , and (v t , ŝt ) can be estimated as the optimum of the following problem: where Û t-1 is the estimated subspace at time point t -1. The above problem does not have an explicit solution but can be iteratively solved by alternatively optimizing s t and v t . Approximating the solution with a one-step update from a reasonable initial point ŝt = s t , we have: For a small λ 1 , we have the approximations vt = ( where represents the projection to the orthogonal complement subspace of Û t-1 . A good choice of λ 1 is of the order of O(1/ max(m, n win )), and λ 1 is small when max(m, n win ) is large. (We will discuss the tuning of parameters in more details later.) Under the mild assumption that the nonzero elements of s t are not too small (that is: This analysis also provides us a clear mathematical description of the term \"abruptly changed subspace\", i.e., for a piecewise constant subspace, we say that a change point exists at time t when the vector P U ⊥ t-1 U t v t is not close to zero. Based on the above observations, we propose to monitor ĉt as a change point detection algorithm. The algorithm determines that a change point exists when it finds ĉt is abnormally high for a while. There are two user-given parameters: N check and α prop . If the algorithm finds more than α prop observations with abnormally high ĉt in N check consecutive observations, the algorithm detects a change point and traces back to find the location of the change point. We refer to the cases where the underlying estimated subspace approximates the true subspace well as \"normal\", otherwise we refer to it as \"abnormal\". All the collected information of {ĉ j } t-1 j=1 from the normal periods is stored in H c ∈ IR m+1 , where the (i + 1)-th element of H c equals the number of times that {ĉ j } is i. For calculating ĉt from a new observation m t , based on H c we can flag this observation as normal or an abnormal via hypothesis testing. The p-value is computed as which is the probability for observing a sparse vector with at least as many nonzero elements as the current observation under the hypothesis that this observation is normal. If p ≤ α, the observation is flagged as abnormal, otherwise, this observation is regarded as normal. Here, α is a user-specified threshold and α = 0.01 is suggested. The pseudocode of OMWRPCA-CP is displayed in Algorithm 3. Here are the key steps: 1) Initialize H c as 0 m+1 , the buffer list for flag B f and the buffer list for count B c as empty lists. Tuning the parameters properly is important to the success of the proposed algorithms. One can select (λ 1 , λ 2 ) based on crossvalidation. A rule of thumb choice is λ 1 = 1/ max(m, n win ) and λ 2 = 100/ max(m, n win ). N check needs to be kept smaller than n win /2 to avoid missing a change point, yet not too small to avoid generating false alarms. α prop can be chosen based on the user's prior knowledge. Sometimes the assumption that the support size of sparse vector s t remains stable and much smaller than m is violated in real-world data. For example, in video surveillance data, the foreground might contain significant variations over time. In this case, we add one additional positive tuning parameter n tol and modify the formula of p-value which makes the hypothesis testing more conservative and forces the algorithm to detect fewer change points reducing false alarms. It is easy to prove that OMWRPCA and OMWRPCA-CP (ignoring the burn-in samples training) have the same computational complexity as STOC-RPCA. The computational cost of each new observation is O(mr 2 ), which is independent of the sample size and linear in the dimension of observation [30]. In contrast, RPCA-PCP computes an SVD and a thresholding operation in each iteration with the computational complexity O(nm 2 ). The memory cost of RPCA-PCP is O(mn) and that of OMWRPCA and OMWRPCA-CP are O(mr) and O(mr + N check ), respectively. Generally, the proposed methods are well suitable to process big data. Lastly, the current OMWRPCA-CP does hypothesis testing based on all of the historical information of ĉt . We can easily change H c to a queue structure which only stores the recent history of ĉt . The user can specify how far in history to trace back. This change makes the algorithm detect a change point only by comparisons with recent history. Since the modification is straightforward, we do not present it here. In this section, the proposed OMWRPCA algorithm will be evaluated by comparison with RPCA-STOC and NORST on extensive numerical experiments and an application to a realworld video surveillance data. To make a fair comparison between OMWRPCA and RPCA-STOC, we estimate the rank r in RPCA-STOC with burn-in samples following the same steps as OMWRPCA. For NORST, we use the code downloaded from https://github.com/praneethmurthy/NORST. All algorithms are implemented in Matlab and our code is available at https://github.com/wxiao0421/onlineRPCA.git. We choose the parameters λ 1 = 1/ √ 400, λ 2 = 100/ √ 400, n win = 200, n burnin = 200, n cp-burnin = 200, n test = 100, N check = 20, α prop = 0.5, α = 0.01, n positive = 3, n tol = 0 for OMWRPCA. We do a grid search and choose the best parameters for NORST (ω evals = 0.002, α = 150, K = 4, ω supp = 20) as the default parameters suggested in the paper do not work well. The first simulation study is similar to the setting in [30]. The observations are generated through M = L + S, where S is a sparse matrix with a fraction of ρ non-zero elements. The non-zero elements of S are randomly chosen and generated from a uniform distribution over the interval of [-1000, 1000]. The low-rank subspace L is generated as a product L = U V , where the sizes of U and V are m × r and r × T , respectively. The elements of both U and V are i.i.d. samples from N (0, 1). Here U is the basis of the constant subspace with dimension r. We fix T = 5000 and m = 400. Burn-in samples M b of size 400 × 200 are generated and (r, ρ) are given values (10, 0.01), (10,0.1), (50,0.01), and (50,0.1). For each setting, we run 50 replications. We compare STOC-RPCA, OMWRPCA, OMWRPCA-CP, and NORST on the following three criteria: Here ERR L is the relative error of the low-rank matrix L, ERR S is the relative error of the sparse matrix S, and F S is the proportion of incorrectly identified elements in S. Box plots of ERR L , ERR S , F S along with the running times are shown in Fig. 2. OMWRPCA-CP has the same result as OMWRPCA, and no change point is detected by OMWRPCA-CP in all replications. NORST falsely detected 6 equally spaced change points 750 observations apart. In fact, it adds one more change point whenever the algorithm switches to detect phase. All the three methods STOC-RPCA, OMWRPCA, and OMWRPCA-CP have comparable performance on ERR S and running times. STOC-RPCA has slightly better performance on ERR L when ρ = 0.1. NORST performs better than other three methods. All methods take approximately the same amount of time to run, and are all very fast (less than 0.5 minutes per replication for all settings). In contrast, MWRPCA takes around 100 minutes for the settings (r, ρ) = (10, 0.01), (10,0.1), (50,0.01), and more than 1000 minutes for the setting (r, ρ) = (50, 0.1). In the next experiment, we adopt almost the same setting as previously described except that the underlying subspace U changes linearly over time. We first generate U 0 ∈ IR m×r  with i.i.d. samples from N (0, 1) and then generate burn-in samples M b based on U 0 . U slowly changes over time by adding new matrices { Ũ k } K k=1 that are generated independently with i.i.d. N (0, 1) elements to the first r 0 columns of U , where Ũ k ∈ IR m×r 0 , k = 1, . . . , K and K = T /T p , where T p = 250. Specifically, for t = T p * i + j, where i = 0, . . . , K -1, j = 0, . . . , T p -1, we have: In this simulation, r 0 = 5. The comparison result displayed in Fig. 3 shows that OMWRPCA, OMWRPCA-CP, and NORST outperform STOC-RPCA which is not able to efficiently track changing subspaces. NORST is slightly better than OMWRPCA-CP and OMWRPCA. OMWRPCA-CP has the same result as OMWR-PCA, and no change point is detected. Again, NORST detects 6 equally spaced change points, which is reasonable since NORST assumes piecewise constant subspaces. In Fig. 4, we plot the average of ERR L across all replications as a function of number of observations. It verifies that the performance of STOC-RPCA deteriorates over time, while that of OMWRPCA and OMWRPCA-CP is quite stable. Based on the previous experiments, we further add two change points at time points 1000 and 2000, i.e., the underlying subspace U is changed and generated completely independently. These two change points cut the timeline into three sections, and in section i the subspace U starts with rank r i . Let r = (r 1 , r 2 , r 3 ) T . We consider three settings of r: (10, 10, 10) T , (50, 50, 50) T and (10, 50, 25) T . The subspace U slowly changes over time in each section as assumed in numerical experiment B, where T p = 250. In this experiment, T = 3000. The box plots of ERR L , ERR S , and F S at time point T are shown in Fig. 5. Under almost all settings, OMWRPCA outperforms STOC-RPCA. OMWRPCA-CP has the best performance among these three methods. Fig. 6 plots the average of ERR L across all replications as a function of time t to investigate the progress of performance across different methods. The performance of both STOC-RPCA and OMWRPCA deteriorate quickly after the first change point at t = 1000, while the performance of OMWRPCA-CP remains stable, indicating that OMWRPCA-CP can track subspaces correctly under the scenario of a suddenly changed subspace. NORST outperforms OMWRPCA-CP in the case r = (10, 10, 10) T where all the subspaces have the same dimension, but underperforms OMWRPCA-CP in the case when r = (10, 50, 25) T where there is a sharp change in dimensions. Furthermore, OMWRPCA-CP correctly identifies two change points for all replications over all settings. The distribution of the difference between the detected change points and the true change points (δ cp = tcpt cp ) is given in Fig. 7. We observe that δ cp is typically 0 when the subspaces before and after the change point are highly distinguishable, which represents the cases r = (50, 50, 50) T and (10, 50, 25) T . The performance when the change in the subspace is not dramatic (r = (10, 10, 10) T ) is also reasonably good. NORST fails to correctly identify the change points. Low-rank subspace tracking is suitable for surveillance videos because successive frames are highly correlated [6]. In a surveillance video the background is generally stable and might change very slowly due to varying illumination. In this paper, experiments on airport and lobby surveillance video data [6], [7], [38] and the UCSD anomaly detection dataset 1 are considered. To evaluate the effectiveness of OMWRPCA-CP for tracking a slowly changing subspace with change points, we pan a \"virtual camera\" moving from left to right and from right to left through the video. The \"virtual camera\" moves at a speed of 1 pixel per 10 frames. The original frame has size 176 × 144, 160 × 128 and 238 × 158 for the airport, the lobby, and the UCSD video data, respectively. The virtual camera has the same height and half the width. We stack each frame to a column and feed it to the algorithms. To make the background subtraction task more difficult, we add one change point to both videos where the \"virtual camera\" jumps instantly from the right-hand most side to the left-hand most side. 2  We choose the following parameters in the algorithm, λ recovered low-rank L and sparse Ŝ compared with STOC_RPCA and NORST. To investigate how the window size affects the final result, we conduct an experiment with different window sizes. Consider data generated as in Section IV-C and set m = 400, n = 4000, r = [10,10,10], ρ = 0.1. The performance of OMWRPCA-CP with different window sizes is reported in Table I, from which one can observe that the recovery accuracy is quite stable over a relatively large range. As expected, a larger window size implies a longer running time. Besides this key parameter, there are other parameters in OMWRPCA-CP. Generally, the performance is not very sensitive to them, and the users could tune them according to the suggestions in the previous experiments. In this paper we have proposed an online robust PCA algorithm that can track both slowly and abruptly changed subspaces. By embedding hypothesis testing in the algorithm one can discover the exact locations of change points for the underlying low-rank subspaces. In numerical experiments, the proposed algorithm shows promising performance for tracking slowly changing subspaces with change points in an online fashion. The proposed algorithm is promising for real-time video layering where the video sequence is separated into a slowly changing background and a sparse foreground, which has been evaluated in this paper, as well as in failure detection in mechanical systems, intrusion detection in computer networks, and human activity recognition based on sensor data, which are left for future work. m×t to denote the matrix of observed samples until time t. Let M , L, S denotes M T , L T , S T respectively. m×t c on the left-hand side. If c is the number popped from B c then updateH c with c (H c [c + 1] ← H c [c + 1] + 1). If n b < N check +1, there is no update. The buffer size n b is kept within N check . When the buffer size reaches N check , it remains in N check . Buffers B f contain flag information of the most recent N check observations and is used to detect change points. 6) positive consecutive abnormal cases. For example, suppose B f [i] corresponding to time points t 0 , t 0 + 1, . . ., t 0 + n positive -1 is the first instance of n positive consecutive abnormal cases. The change point is determined as t 0 . Here n positive is a parameter specified by the user. In practice, n positive = 3 is suggested. After identifying the change point, OMWRPCA-CP restarts from it. c on the left-hand side. If c is the number popped from B c then updateH c with c (H c [c + 1] ← H c [c + 1] + 1). If n b < N check + c on the left-hand side. If c is the number popped from B c then update Authorized licensed use limited to: Queen Mary University of London. Downloaded on March 23,2025 at 00:27:32 UTC from IEEE Xplore. Restrictions apply.",
  "filePath": "C:\\Users\\Alex Isaev\\Documents\\truth source\\citation-verifier\\src\\document-database\\documents\\online_robust_principal_component_analysis_with_change_point_detection.json",
  "sourcePdf": "C:\\Users\\Alex Isaev\\Documents\\truth source\\citation-verifier\\src\\document-database\\pdf-documents\\28.pdf",
  "doi": "10.1109/TMM.2019.2923097",
  "year": "",
  "journal": ""
}